
@article{ailon_fast_2009,
	title = {The {Fast} {Johnson}–{Lindenstrauss} {Transform} and {Approximate} {Nearest} {Neighbors}},
	volume = {39},
	issn = {0097-5397, 1095-7111},
	url = {http://epubs.siam.org/doi/10.1137/060673096},
	doi = {10.1137/060673096},
	language = {en},
	number = {1},
	urldate = {2020-03-06},
	journal = {SIAM Journal on Computing},
	author = {Ailon, Nir and Chazelle, Bernard},
	month = jan,
	year = {2009},
	pages = {302--322},
	file = {Ailon and Chazelle - 2009 - The Fast Johnson–Lindenstrauss Transform and Appro.pdf:C\:\\Users\\vamsi\\Zotero\\storage\\Q3CGYH49\\Ailon and Chazelle - 2009 - The Fast Johnson–Lindenstrauss Transform and Appro.pdf:application/pdf}
}

@article{woodruff_sketching_2014,
	title = {Sketching as a {Tool} for {Numerical} {Linear} {Algebra}},
	volume = {10},
	issn = {1551-305X, 1551-3068},
	url = {http://arxiv.org/abs/1411.4357},
	doi = {10.1561/0400000060},
	abstract = {This survey highlights the recent advances in algorithms for numerical linear algebra that have come from the technique of linear sketching, whereby given a matrix, one ﬁrst compresses it to a much smaller matrix by multiplying it by a (usually) random matrix with certain properties. Much of the expensive computation can then be performed on the smaller matrix, thereby accelerating the solution for the original problem. In this survey we consider least squares as well as robust regression problems, low rank approximation, and graph sparsiﬁcation. We also discuss a number of variants of these problems. Finally, we discuss the limitations of sketching methods.},
	language = {en},
	number = {1-2},
	urldate = {2020-03-06},
	journal = {Foundations and Trends® in Theoretical Computer Science},
	author = {Woodruff, David P.},
	year = {2014},
	note = {arXiv: 1411.4357},
	keywords = {Computer Science - Data Structures and Algorithms},
	pages = {1--157},
	annote = {Comment: fixed minor errors/typos in section 4.3, e.g., Fact 6 and its propagation, clarified when Lemma 4.2 can be applied, typos in section 4.2.3 (G should be applied on the left), other typos throughout},
	file = {Woodruff - 2014 - Sketching as a Tool for Numerical Linear Algebra.pdf:C\:\\Users\\vamsi\\Zotero\\storage\\592HGQUR\\Woodruff - 2014 - Sketching as a Tool for Numerical Linear Algebra.pdf:application/pdf}
}

@article{drineas_fast_2006,
	title = {Fast {Monte} {Carlo} {Algorithms} for {Matrices} {II}: {Computing} a {Low}-{Rank} {Approximation} to a {Matrix}},
	volume = {36},
	issn = {0097-5397, 1095-7111},
	shorttitle = {Fast {Monte} {Carlo} {Algorithms} for {Matrices} {II}},
	url = {http://epubs.siam.org/doi/10.1137/S0097539704442696},
	doi = {10.1137/S0097539704442696},
	language = {en},
	number = {1},
	urldate = {2020-03-06},
	journal = {SIAM Journal on Computing},
	author = {Drineas, Petros and Kannan, Ravi and Mahoney, Michael W.},
	month = jan,
	year = {2006},
	pages = {158--183},
	file = {Drineas et al. - 2006 - Fast Monte Carlo Algorithms for Matrices II Compu.pdf:C\:\\Users\\vamsi\\Zotero\\storage\\8RSIYUR5\\Drineas et al. - 2006 - Fast Monte Carlo Algorithms for Matrices II Compu.pdf:application/pdf}
}

@article{drineas_faster_2011,
	title = {Faster least squares approximation},
	volume = {117},
	issn = {0029-599X, 0945-3245},
	url = {http://link.springer.com/10.1007/s00211-010-0331-6},
	doi = {10.1007/s00211-010-0331-6},
	abstract = {Least squares approximation is a technique to ﬁnd an approximate solution to a system of linear equations that has no exact solution. In a typical setting, one lets n be the number of constraints and d be the number of variables, with n d. Then, existing exact methods ﬁnd a solution vector in O(nd2) time. We present two randomized algorithms that provide accurate relative-error approximations to the optimal value and the solution vector of a least squares approximation problem more rapidly than existing exact algorithms. Both of our algorithms preprocess the data with the Randomized Hadamard transform. One then uniformly randomly samples constraints and solves the smaller problem on those constraints, and the other performs a sparse random projection and solves the smaller problem on those projected coordinates. In both cases, solving the smaller problem provides relative-error approximations, and, if n is sufﬁciently larger than d, the approximate solution can be computed in O(nd ln d) time.},
	language = {en},
	number = {2},
	urldate = {2020-03-06},
	journal = {Numerische Mathematik},
	author = {Drineas, Petros and Mahoney, Michael W. and Muthukrishnan, S. and Sarlós, Tamás},
	month = feb,
	year = {2011},
	pages = {219--249},
	file = {Drineas et al. - 2011 - Faster least squares approximation.pdf:C\:\\Users\\vamsi\\Zotero\\storage\\PM6ZPASF\\Drineas et al. - 2011 - Faster least squares approximation.pdf:application/pdf}
}

@inproceedings{sarlos_improved_2006,
	address = {Berkeley, CA},
	title = {Improved {Approximation} {Algorithms} for {Large} {Matrices} via {Random} {Projections}},
	isbn = {978-0-7695-2720-8},
	url = {https://ieeexplore.ieee.org/document/4031351/},
	doi = {10.1109/FOCS.2006.37},
	abstract = {Recently several results appeared that show signiﬁcant reduction in time for matrix multiplication, singular value decomposition as well as linear ( 2) regression, all based on data dependent random sampling. Our key idea is that low dimensional embeddings can be used to eliminate data dependence and provide more versatile, linear time pass efﬁcient matrix computation. Our main contribution is summarized as follows.},
	language = {en},
	urldate = {2020-03-06},
	booktitle = {2006 47th {Annual} {IEEE} {Symposium} on {Foundations} of {Computer} {Science} ({FOCS}'06)},
	publisher = {IEEE},
	author = {Sarlos, Tamas},
	month = oct,
	year = {2006},
	pages = {143--152},
	file = {Sarlos - 2006 - Improved Approximation Algorithms for Large Matric.pdf:C\:\\Users\\vamsi\\Zotero\\storage\\GYLJA2EX\\Sarlos - 2006 - Improved Approximation Algorithms for Large Matric.pdf:application/pdf}
}

@inproceedings{boutsidis_improved_2009,
	title = {An {Improved} {Approximation} {Algorithm} for the {Column} {Subset} {Selection} {Problem}},
	isbn = {978-0-89871-680-1 978-1-61197-306-8},
	url = {https://epubs.siam.org/doi/10.1137/1.9781611973068.105},
	doi = {10.1137/1.9781611973068.105},
	language = {en},
	urldate = {2020-03-06},
	booktitle = {Proceedings of the {Twentieth} {Annual} {ACM}-{SIAM} {Symposium} on {Discrete} {Algorithms}},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Boutsidis, Christos and Mahoney, Michael W. and Drineas, Petros},
	month = jan,
	year = {2009},
	pages = {968--977},
	file = {Boutsidis et al. - 2009 - An Improved Approximation Algorithm for the Column.pdf:C\:\\Users\\vamsi\\Zotero\\storage\\2MTIS6TT\\Boutsidis et al. - 2009 - An Improved Approximation Algorithm for the Column.pdf:application/pdf}
}

@article{mahoney_cur_2009,
	title = {{CUR} matrix decompositions for improved data analysis},
	volume = {106},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.0803205106},
	doi = {10.1073/pnas.0803205106},
	language = {en},
	number = {3},
	urldate = {2020-03-06},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Mahoney, Michael W. and Drineas, Petros},
	month = jan,
	year = {2009},
	pages = {697--702},
	file = {Mahoney and Drineas - 2009 - CUR matrix decompositions for improved data analys.pdf:C\:\\Users\\vamsi\\Zotero\\storage\\G4QQ77UP\\Mahoney and Drineas - 2009 - CUR matrix decompositions for improved data analys.pdf:application/pdf}
}

@article{boutsidis_improved_2013,
	title = {Improved matrix algorithms via the {Subsampled} {Randomized} {Hadamard} {Transform}},
	url = {http://arxiv.org/abs/1204.0062},
	abstract = {Several recent randomized linear algebra algorithms rely upon fast dimension reduction methods. A popular choice is the Subsampled Randomized Hadamard Transform (SRHT). In this article, we address the eﬃcacy, in the Frobenius and spectral norms, of an SRHT-based low-rank matrix approximation technique introduced by Woolfe, Liberty, Rohklin, and Tygert. We establish a slightly better Frobenius norm error bound than currently available, and a much sharper spectral norm error bound (in the presence of reasonable decay of the singular values). Along the way, we produce several results on matrix operations with SRHTs (such as approximate matrix multiplication) that may be of independent interest. Our approach builds upon Tropp’s in “Improved analysis of the Subsampled Randomized Hadamard Transform”.},
	language = {en},
	urldate = {2020-03-06},
	journal = {arXiv:1204.0062 [cs, math]},
	author = {Boutsidis, Christos and Gittens, Alex},
	month = jun,
	year = {2013},
	note = {arXiv: 1204.0062},
	keywords = {Computer Science - Data Structures and Algorithms, Mathematics - Numerical Analysis},
	annote = {Comment: to appear in SIAM Journal on Matrix Analysis and Applications},
	file = {Boutsidis and Gittens - 2013 - Improved matrix algorithms via the Subsampled Rand.pdf:C\:\\Users\\vamsi\\Zotero\\storage\\QQ7CIAEH\\Boutsidis and Gittens - 2013 - Improved matrix algorithms via the Subsampled Rand.pdf:application/pdf}
}

@incollection{beals_extensions_1984,
	address = {Providence, Rhode Island},
	title = {Extensions of {Lipschitz} mappings into a {Hilbert} space},
	volume = {26},
	isbn = {978-0-8218-5030-5 978-0-8218-7611-4},
	url = {http://www.ams.org/conm/026/},
	language = {en},
	urldate = {2020-03-06},
	booktitle = {Contemporary {Mathematics}},
	publisher = {American Mathematical Society},
	author = {Johnson, William B. and Lindenstrauss, Joram},
	editor = {Beals, Richard and Beck, Anatole and Bellow, Alexandra and Hajian, Arshag},
	year = {1984},
	doi = {10.1090/conm/026/737400},
	pages = {189--206},
	file = {Johnson and Lindenstrauss - 1984 - Extensions of Lipschitz mappings into a Hilbert sp.pdf:C\:\\Users\\vamsi\\Zotero\\storage\\THLA2W3C\\Johnson and Lindenstrauss - 1984 - Extensions of Lipschitz mappings into a Hilbert sp.pdf:application/pdf}
}

@article{xu_comprehensive_2015,
	title = {A {Comprehensive} {Survey} of {Clustering} {Algorithms}},
	volume = {2},
	issn = {2198-5804, 2198-5812},
	url = {http://link.springer.com/10.1007/s40745-015-0040-1},
	doi = {10.1007/s40745-015-0040-1},
	abstract = {Data analysis is used as a common method in modern science research, which is across communication science, computer science and biology science. Clustering, as the basic composition of data analysis, plays a signiﬁcant role. On one hand, many tools for cluster analysis have been created, along with the information increase and subject intersection. On the other hand, each clustering algorithm has its own strengths and weaknesses, due to the complexity of information. In this review paper, we begin at the deﬁnition of clustering, take the basic elements involved in the clustering process, such as the distance or similarity measurement and evaluation indicators, into consideration, and analyze the clustering algorithms from two perspectives, the traditional ones and the modern ones. All the discussed clustering algorithms will be compared in detail and comprehensively shown in Appendix Table 22.},
	language = {en},
	number = {2},
	urldate = {2020-03-06},
	journal = {Annals of Data Science},
	author = {Xu, Dongkuan and Tian, Yingjie},
	month = jun,
	year = {2015},
	pages = {165--193},
	file = {Xu and Tian - 2015 - A Comprehensive Survey of Clustering Algorithms.pdf:C\:\\Users\\vamsi\\Zotero\\storage\\548EZ3U3\\Xu and Tian - 2015 - A Comprehensive Survey of Clustering Algorithms.pdf:application/pdf}
}

@article{jarquin_reaction_2014,
	title = {A reaction norm model for genomic selection using high-dimensional genomic and environmental data},
	volume = {127},
	issn = {0040-5752, 1432-2242},
	url = {http://link.springer.com/10.1007/s00122-013-2243-1},
	doi = {10.1007/s00122-013-2243-1},
	abstract = {Key message  New methods that incorporate the main and interaction effects of high-dimensional markers and of high-dimensional environmental covariates gave increased prediction accuracy of grain yield in wheat across and within environments.},
	language = {en},
	number = {3},
	urldate = {2020-03-06},
	journal = {Theoretical and Applied Genetics},
	author = {Jarquín, Diego and Crossa, José and Lacaze, Xavier and Du Cheyron, Philippe and Daucourt, Joëlle and Lorgeou, Josiane and Piraux, François and Guerreiro, Laurent and Pérez, Paulino and Calus, Mario and Burgueño, Juan and de los Campos, Gustavo},
	month = mar,
	year = {2014},
	pages = {595--607},
	file = {Jarquín et al. - 2014 - A reaction norm model for genomic selection using .pdf:C\:\\Users\\vamsi\\Zotero\\storage\\68J74Q29\\Jarquín et al. - 2014 - A reaction norm model for genomic selection using .pdf:application/pdf}
}

@article{noauthor_bglr_nodate,
	title = {{BGLR}: an {R}-package for {Whole}-{Genome} {Regression}},
	abstract = {Many modern genomic data analysis problems require implementing regressions where the number of unknowns (p, e.g., the number of marker eﬀects) vastly exceeds sample size (n). Implementing these large-p-with-small-n regressions poses several statistical and computational challenges. Some of these challenges can be confronted using Bayesian methods, and the Bayesian approach allows integrating various parametric and non-parametric shrinkage and variable selection procedures in a uniﬁed and consistent manner. The BGLR R-package implements a large collection Bayesian regression models, including various parametric regressions where regression coeﬃcients are allowed to have diﬀerent types of prior densities (ﬂat, normal, scaled-t, double-exponential and various ﬁnite mixtures of the spike-slab family) and semi-parametric methods (Bayesian reproducing kernel Hilbert spaces nregressions, RKHS). The software was originally developed as an extension of the BLR package and with a focus on genomic applications; however, the methods implemented are useful for many non-genomic applications as well. The response can be continuous (censored or not) or categorical (either binary, or ordinal). The algorithm is based on a Gibbs Sampler with scalar updates and the implementation takes advantage of eﬃcient compiled C and Fortran routines. In this article we describe the methods implemented in BGLR, present examples of the use of the package and discuss practical issues emerging in real-data analysis.},
	language = {en},
	pages = {30},
	file = {BGLR an R-package for Whole-Genome Regression.pdf:C\:\\Users\\vamsi\\Zotero\\storage\\678TGRPE\\BGLR an R-package for Whole-Genome Regression.pdf:application/pdf}
}

@article{roorkiwal_genomic-enabled_2018,
	title = {Genomic-enabled prediction models using multi-environment trials to estimate the effect of genotype × environment interaction on prediction accuracy in chickpea},
	volume = {8},
	issn = {2045-2322},
	url = {http://www.nature.com/articles/s41598-018-30027-2},
	doi = {10.1038/s41598-018-30027-2},
	language = {en},
	number = {1},
	urldate = {2020-03-06},
	journal = {Scientific Reports},
	author = {Roorkiwal, Manish and Jarquin, Diego and Singh, Muneendra K. and Gaur, Pooran M. and Bharadwaj, Chellapilla and Rathore, Abhishek and Howard, Reka and Srinivasan, Samineni and Jain, Ankit and Garg, Vanika and Kale, Sandip and Chitikineni, Annapurna and Tripathi, Shailesh and Jones, Elizabeth and Robbins, Kelly R. and Crossa, Jose and Varshney, Rajeev K.},
	month = dec,
	year = {2018},
	pages = {11701},
	file = {Roorkiwal et al. - 2018 - Genomic-enabled prediction models using multi-envi.pdf:C\:\\Users\\vamsi\\Zotero\\storage\\KC9YYZ3L\\Roorkiwal et al. - 2018 - Genomic-enabled prediction models using multi-envi.pdf:application/pdf}
}

@article{jarquin_increasing_2017,
	title = {Increasing {Genomic}-{Enabled} {Prediction} {Accuracy} by {Modeling} {Genotype} × {Environment} {Interactions} in {Kansas} {Wheat}},
	volume = {10},
	issn = {19403372},
	url = {http://doi.wiley.com/10.3835/plantgenome2016.12.0130},
	doi = {10.3835/plantgenome2016.12.0130},
	abstract = {Wheat (Triticum aestivum L.) breeding programs test experimental lines in multiple locations over multiple years to get an accurate assessment of grain yield and yield stability. Selections in early generations of the breeding pipeline are based on information from only one or few locations and thus materials are advanced with little knowledge of the genotype × environment interaction (G × E) effects. Later, large trials are conducted in several locations to assess the performance of more advanced lines across environments. Genomic selection (GS) models that include G × E covariates allow us to borrow information not only from related materials, but also from historical and correlated environments to better predict performance within and across specific environments. We used reaction norm models with several cross-validation schemes to demonstrate the increased breeding efficiency of Kansas State University’s hard red winter wheat breeding program. The GS reaction norm models line effect (L) + environment effect (E), L + E + genotype environment (G), and L + E + G + (G × E) effects) showed high accuracy values ({\textgreater}0.4) when predicting the yield performance in untested environments, sites or both. The GS model L + E + G + (G × E) presented the highest prediction ability (r = 0.54) when predicting yield in incomplete field trials for locations with a moderate number of lines. The difficulty of predicting future years (forward prediction) is indicated by the relatively low accuracy (r = 0.171) seen even when environments with 300+ lines were included.},
	language = {en},
	number = {2},
	urldate = {2020-03-06},
	journal = {The Plant Genome},
	author = {Jarquín, Diego and Lemes da Silva, Cristiano and Gaynor, R. Chris and Poland, Jesse and Fritz, Allan and Howard, Reka and Battenfield, Sarah and Crossa, Jose},
	month = jul,
	year = {2017},
	pages = {plantgenome2016.12.0130},
	file = {Jarquín et al. - 2017 - Increasing Genomic-Enabled Prediction Accuracy by .pdf:C\:\\Users\\vamsi\\Zotero\\storage\\2GFC2LHD\\Jarquín et al. - 2017 - Increasing Genomic-Enabled Prediction Accuracy by .pdf:application/pdf}
}

@article{geppert_random_2017,
	title = {Random projections for {Bayesian} regression},
	volume = {27},
	issn = {0960-3174, 1573-1375},
	url = {http://link.springer.com/10.1007/s11222-015-9608-z},
	doi = {10.1007/s11222-015-9608-z},
	language = {en},
	number = {1},
	urldate = {2020-03-09},
	journal = {Statistics and Computing},
	author = {Geppert, Leo N. and Ickstadt, Katja and Munteanu, Alexander and Quedenfeld, Jens and Sohler, Christian},
	month = jan,
	year = {2017},
	pages = {79--101},
	file = {Full Text:C\:\\Users\\vamsi\\Zotero\\storage\\QSWUDDHY\\Geppert et al. - 2017 - Random projections for Bayesian regression.pdf:application/pdf}
}

@article{de_los_campos_predicting_2009,
	title = {Predicting {Quantitative} {Traits} {With} {Regression} {Models} for {Dense} {Molecular} {Markers} and {Pedigree}},
	volume = {182},
	url = {http://www.genetics.org/content/182/1/375.abstract},
	doi = {10.1534/genetics.109.101501},
	abstract = {The availability of genomewide dense markers brings opportunities and challenges to breeding programs. An important question concerns the ways in which dense markers and pedigrees, together with phenotypic records, should be used to arrive at predictions of genetic values for complex traits. If a large number of markers are included in a regression model, marker-specific shrinkage of regression coefficients may be needed. For this reason, the Bayesian least absolute shrinkage and selection operator (LASSO) (BL) appears to be an interesting approach for fitting marker effects in a regression model. This article adapts the BL to arrive at a regression model where markers, pedigrees, and covariates other than markers are considered jointly. Connections between BL and other marker-based regression models are discussed, and the sensitivity of BL with respect to the choice of prior distributions assigned to key parameters is evaluated using simulation. The proposed model was fitted to two data sets from wheat and mouse populations, and evaluated using cross-validation methods. Results indicate that inclusion of markers in the regression further improved the predictive ability of models. An R program that implements the proposed model is freely available.},
	number = {1},
	journal = {Genetics},
	author = {de los Campos, Gustavo and Naya, Hugo and Gianola, Daniel and Crossa, José and Legarra, Andrés and Manfredi, Eduardo and Weigel, Kent and Cotes, José Miguel},
	month = may,
	year = {2009},
	pages = {375}
}

@article{liberty_randomized_2007,
	title = {Randomized algorithms for the low-rank approximation of matrices},
	volume = {104},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/cgi/doi/10.1073/pnas.0709640104},
	doi = {10.1073/pnas.0709640104},
	language = {en},
	number = {51},
	urldate = {2020-06-17},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Liberty, E. and Woolfe, F. and Martinsson, P.-G. and Rokhlin, V. and Tygert, M.},
	month = dec,
	year = {2007},
	pages = {20167--20172},
	file = {Liberty et al. - 2007 - Randomized algorithms for the low-rank approximati.pdf:C\:\\Users\\vamsi\\Zotero\\storage\\7SA76LGL\\Liberty et al. - 2007 - Randomized algorithms for the low-rank approximati.pdf:application/pdf}
}

@article{papailiopoulos_provable_2014,
	title = {Provable {Deterministic} {Leverage} {Score} {Sampling}},
	url = {http://arxiv.org/abs/1404.1530},
	abstract = {We explain theoretically a curious empirical phenomenon: "Approximating a matrix by deterministically selecting a subset of its columns with the corresponding largest leverage scores results in a good low-rank matrix surrogate". To obtain provable guarantees, previous work requires randomized sampling of the columns with probabilities proportional to their leverage scores. In this work, we provide a novel theoretical analysis of deterministic leverage score sampling. We show that such deterministic sampling can be provably as accurate as its randomized counterparts, if the leverage scores follow a moderately steep power-law decay. We support this power-law assumption by providing empirical evidence that such decay laws are abundant in real-world data sets. We then demonstrate empirically the performance of deterministic leverage score sampling, which many times matches or outperforms the state-of-the-art techniques.},
	urldate = {2020-07-22},
	journal = {arXiv:1404.1530 [cs, math, stat]},
	author = {Papailiopoulos, Dimitris and Kyrillidis, Anastasios and Boutsidis, Christos},
	month = jun,
	year = {2014},
	note = {arXiv: 1404.1530},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Information Theory, Mathematics - Numerical Analysis, Mathematics - Statistics Theory, Statistics - Machine Learning},
	annote = {Comment: 20th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
	file = {arXiv Fulltext PDF:C\:\\Users\\vamsi\\Zotero\\storage\\BG2QLXQN\\Papailiopoulos et al. - 2014 - Provable Deterministic Leverage Score Sampling.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\vamsi\\Zotero\\storage\\SEKKB36R\\1404.html:text/html}
}

@article{ghashami_frequent_2015,
	title = {Frequent {Directions} : {Simple} and {Deterministic} {Matrix} {Sketching}},
	shorttitle = {Frequent {Directions}},
	url = {http://arxiv.org/abs/1501.01711},
	abstract = {We describe a new algorithm called Frequent Directions for deterministic matrix sketching in the row-updates model. The algorithm is presented an arbitrary input matrix \$A {\textbackslash}in R{\textasciicircum}\{n {\textbackslash}times d\}\$ one row at a time. It performed \$O(d {\textbackslash}times {\textbackslash}ell)\$ operations per row and maintains a sketch matrix \$B {\textbackslash}in R{\textasciicircum}\{{\textbackslash}ell {\textbackslash}times d\}\$ such that for any \$k {\textless} {\textbackslash}ell\$ \${\textbackslash}{\textbar}A{\textasciicircum}TA - B{\textasciicircum}TB {\textbackslash}{\textbar}\_2 {\textbackslash}leq {\textbackslash}{\textbar}A - A\_k{\textbackslash}{\textbar}\_F{\textasciicircum}2 / ({\textbackslash}ell-k)\$ and \${\textbackslash}{\textbar}A - {\textbackslash}pi\_\{B\_k\}(A){\textbackslash}{\textbar}\_F{\textasciicircum}2 {\textbackslash}leq {\textbackslash}big(1 + {\textbackslash}frac\{k\}\{{\textbackslash}ell-k\}{\textbackslash}big) {\textbackslash}{\textbar}A-A\_k{\textbackslash}{\textbar}\_F{\textasciicircum}2 \$ . Here, \$A\_k\$ stands for the minimizer of \${\textbackslash}{\textbar}A - A\_k{\textbackslash}{\textbar}\_F\$ over all rank \$k\$ matrices (similarly \$B\_k\$) and \${\textbackslash}pi\_\{B\_k\}(A)\$ is the rank \$k\$ matrix resulting from projecting \$A\$ on the row span of \$B\_k\$. We show both of these bounds are the best possible for the space allowed. The summary is mergeable, and hence trivially parallelizable. Moreover, Frequent Directions outperforms exemplar implementations of existing streaming algorithms in the space-error tradeoff.},
	urldate = {2020-08-13},
	journal = {arXiv:1501.01711 [cs]},
	author = {Ghashami, Mina and Liberty, Edo and Phillips, Jeff M. and Woodruff, David P.},
	month = apr,
	year = {2015},
	note = {arXiv: 1501.01711},
	keywords = {68W40 (Primary), Computer Science - Data Structures and Algorithms},
	annote = {Comment: 28 pages , This paper contains Frequent Directions algorithm (see arXiv:1206.0594) and relative error bound on it (see arXiv:1307.7454)},
	file = {arXiv Fulltext PDF:C\:\\Users\\vamsi\\Zotero\\storage\\NJ2BJDYE\\Ghashami et al. - 2015 - Frequent Directions  Simple and Deterministic Mat.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\vamsi\\Zotero\\storage\\S7GBK282\\1501.html:text/html}
}

@phdthesis{musco_power_2018,
	type = {Thesis},
	title = {The power of randomized algorithms : from numerical linear algebra to biological systems},
	copyright = {MIT theses are protected by copyright. They may be viewed, downloaded, or printed from this source but further reproduction or distribution in any format is prohibited without written permission.},
	shorttitle = {The power of randomized algorithms},
	url = {https://dspace.mit.edu/handle/1721.1/120424},
	abstract = {In this thesis we study simple, randomized algorithms from a dual perspective. The first part of the work considers how randomized methods can be used to accelerate the solution of core problems in numerical linear algebra. In particular, we give a randomized low-rank approximation algorithm for positive semidefinite matrices that runs in sublinear time, significantly improving upon what is possible with traditional deterministic methods. We also discuss lower bounds on low-rank approximation and spectral summarization problems that attempt to explain the importance of randomization and approximation in accelerating linear algebraic computation. The second part of the work considers how the theory of randomized algorithms can be used more generally as a tool to understand how complexity emerges from low-level stochastic behavior in biological systems. We study population density- estimation in ant colonies, which is a key primitive in social decision-making and task allocation. We define a basic computational model and show how agents in this model can estimate their density using a simple random-walk-based algorithm. We also consider simple randomized algorithms for computational primitives in spiking neural networks, focusing on fast winner-take-all networks.},
	language = {eng},
	urldate = {2020-09-03},
	school = {Massachusetts Institute of Technology},
	author = {Musco, Cameron N. (Cameron Nicholas)},
	year = {2018},
	note = {Accepted: 2019-02-14T15:50:19Z
ISBN: 9781084478763
Journal Abbreviation: From numerical linear algebra to biological systems},
	file = {Full Text PDF:C\:\\Users\\vamsi\\Zotero\\storage\\FXSE3PTC\\Musco - 2018 - The power of randomized algorithms  from numerica.pdf:application/pdf;Snapshot:C\:\\Users\\vamsi\\Zotero\\storage\\6SZAAYVE\\120424.html:text/html}
}

@article{hoerl_ridge_1970,
	title = {Ridge {Regression}: {Biased} {Estimation} for {Nonorthogonal} {Problems}},
	volume = {12},
	issn = {0040-1706, 1537-2723},
	shorttitle = {Ridge {Regression}},
	url = {http://www.tandfonline.com/doi/abs/10.1080/00401706.1970.10488634},
	doi = {10.1080/00401706.1970.10488634},
	language = {en},
	number = {1},
	urldate = {2020-09-16},
	journal = {Technometrics},
	author = {Hoerl, Arthur E. and Kennard, Robert W.},
	month = feb,
	year = {1970},
	pages = {55--67},
	file = {Hoerl and Kennard - 1970 - Ridge Regression Biased Estimation for Nonorthogo.pdf:C\:\\Users\\vamsi\\Zotero\\storage\\ZMRZ8FYR\\Hoerl and Kennard - 1970 - Ridge Regression Biased Estimation for Nonorthogo.pdf:application/pdf}
}

@article{zou_regularization_2005,
	title = {Regularization and variable selection via the elastic net},
	volume = {67},
	issn = {1369-7412, 1467-9868},
	url = {http://doi.wiley.com/10.1111/j.1467-9868.2005.00503.x},
	doi = {10.1111/j.1467-9868.2005.00503.x},
	abstract = {We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efﬁciently, much like algorithm LARS does for the lasso.},
	language = {en},
	number = {2},
	urldate = {2020-09-16},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Zou, Hui and Hastie, Trevor},
	month = apr,
	year = {2005},
	pages = {301--320},
	file = {Zou and Hastie - 2005 - Regularization and variable selection via the elas.pdf:C\:\\Users\\vamsi\\Zotero\\storage\\RTDLKHPX\\Zou and Hastie - 2005 - Regularization and variable selection via the elas.pdf:application/pdf}
}

@article{eckart_approximation_1936,
	title = {The approximation of one matrix by another of lower rank},
	volume = {1},
	issn = {0033-3123, 1860-0980},
	url = {http://link.springer.com/10.1007/BF02288367},
	doi = {10.1007/BF02288367},
	language = {en},
	number = {3},
	urldate = {2020-10-20},
	journal = {Psychometrika},
	author = {Eckart, Carl and Young, Gale},
	month = sep,
	year = {1936},
	pages = {211--218},
	file = {Eckart and Young - 1936 - The approximation of one matrix by another of lowe.pdf:C\:\\Users\\vamsi\\Zotero\\storage\\3L8QWH6A\\Eckart and Young - 1936 - The approximation of one matrix by another of lowe.pdf:application/pdf}
}

@article{mdrsky_symmetric_nodate,
	title = {{SYMMETRIC} {GAUGE} {FUNCTIONS} {AND} {UNITARILY} {INVARIANT} {NORMS}},
	language = {en},
	author = {Mdrsky, L},
	pages = {10},
	file = {Mdrsky - SYMMETRIC GAUGE FUNCTIONS AND UNITARILY INVARIANT .pdf:C\:\\Users\\vamsi\\Zotero\\storage\\RM4JPIP2\\Mdrsky - SYMMETRIC GAUGE FUNCTIONS AND UNITARILY INVARIANT .pdf:application/pdf}
}

@book{golub_matrix_2013,
	address = {Baltimore},
	edition = {Fourth edition},
	series = {Johns {Hopkins} studies in the mathematical sciences},
	title = {Matrix computations},
	isbn = {978-1-4214-0794-4},
	language = {en},
	publisher = {The Johns Hopkins University Press},
	author = {Golub, Gene H. and Van Loan, Charles F.},
	year = {2013},
	note = {OCLC: ocn824733531},
	keywords = {Data processing, Matrices},
	file = {Golub and Van Loan - 2013 - Matrix computations.pdf:C\:\\Users\\vamsi\\Zotero\\storage\\SCW4L397\\Golub and Van Loan - 2013 - Matrix computations.pdf:application/pdf}
}

@misc{noauthor_computational_2020,
	title = {Computational complexity of mathematical operations},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Computational_complexity_of_mathematical_operations\&oldid=984815033},
	abstract = {The following tables list the computational complexity of various algorithms for common mathematical operations.
Here, complexity refers to the time complexity of performing computations on a multitape Turing machine. See big O notation for an explanation of the notation used.
Note: Due to the variety of multiplication algorithms, 
  
    
      
        M
        (
        n
        )
      
    
    \{{\textbackslash}displaystyle M(n)\}
   below stands in for the complexity of the chosen multiplication algorithm.},
	language = {en},
	urldate = {2020-11-10},
	journal = {Wikipedia},
	month = oct,
	year = {2020},
	note = {Page Version ID: 984815033},
	file = {Snapshot:C\:\\Users\\vamsi\\Zotero\\storage\\7L3F2A2N\\index.html:text/html}
}

@article{meuwissen_prediction_2001,
	title = {Prediction of total genetic value using genome-wide dense marker maps.},
	volume = {157},
	issn = {0016-6731},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1461589/},
	abstract = {Recent advances in molecular genetic techniques will make dense marker maps available and genotyping many individuals for these markers feasible. Here we attempted to estimate the effects of approximately 50,000 marker haplotypes simultaneously from a limited number of phenotypic records. A genome of 1000 cM was simulated with a marker spacing of 1 cM. The markers surrounding every 1-cM region were combined into marker haplotypes. Due to finite population size N(e) = 100, the marker haplotypes were in linkage disequilibrium with the QTL located between the markers. Using least squares, all haplotype effects could not be estimated simultaneously. When only the biggest effects were included, they were overestimated and the accuracy of predicting genetic values of the offspring of the recorded animals was only 0.32. Best linear unbiased prediction of haplotype effects assumed equal variances associated to each 1-cM chromosomal segment, which yielded an accuracy of 0.73, although this assumption was far from true. Bayesian methods that assumed a prior distribution of the variance associated with each chromosome segment increased this accuracy to 0.85, even when the prior was not correct. It was concluded that selection on genetic values predicted from markers could substantially increase the rate of genetic gain in animals and plants, especially if combined with reproductive techniques to shorten the generation interval.},
	number = {4},
	urldate = {2020-12-17},
	journal = {Genetics},
	author = {Meuwissen, T H and Hayes, B J and Goddard, M E},
	month = apr,
	year = {2001},
	pmid = {11290733},
	pmcid = {PMC1461589},
	pages = {1819--1829},
	file = {PubMed Central Full Text PDF:C\:\\Users\\vamsi\\Zotero\\storage\\SMQK7ESN\\Meuwissen et al. - 2001 - Prediction of total genetic value using genome-wid.pdf:application/pdf}
}

@article{bernardo_prospects_2007,
	title = {Prospects for {Genomewide} {Selection} for {Quantitative} {Traits} in {Maize}},
	volume = {47},
	copyright = {© Crop Science Society of America},
	issn = {1435-0653},
	url = {https://acsess.onlinelibrary.wiley.com/doi/abs/10.2135/cropsci2006.11.0690},
	doi = {https://doi.org/10.2135/cropsci2006.11.0690},
	abstract = {The availability of cheap and abundant molecular markers in maize (Zea mays L.) has allowed breeders to ask how molecular markers may best be used to achieve breeding progress, without conditioning the question on how breeding has traditionally been done. Genomewide selection refers to marker-based selection without first identifying a subset of markers with significant effects. Our objectives were to assess the response due to genomewide selection compared with marker-assisted recurrent selection (MARS) and to determine the extent to which phenotyping can be minimized and genotyping maximized in genomewide selection. We simulated genomewide selection by evaluating doubled haploids for testcross performance in Cycle 0, followed by two cycles of selection based on markers. Individuals were genotyped for NM markers, and breeding values associated with each of the NM markers were predicted and were all used in genomewide selection. We found that across different numbers of quantitative trait loci (20, 40, and 100) and levels of heritability, the response to genomewide selection was 18 to 43\% larger than the response to MARS. Responses to selection were maintained when the number of doubled haploids phenotyped and genotyped in Cycle 0 was reduced and the number of plants genotyped in Cycles 1 and 2 was increased. Such schemes that minimize phenotyping and maximize genotyping would be feasible only if the cost per marker data point is reduced to about 2 cents. The convenient but incorrect assumption of equal marker variances led to only a minimal loss in the response to genomewide selection. We conclude that genomewide selection, as a brute-force and black-box procedure that exploits cheap and abundant molecular markers, is superior to MARS in maize.},
	language = {en},
	number = {3},
	urldate = {2020-12-17},
	journal = {Crop Science},
	author = {Bernardo, Rex and Yu, Jianming},
	year = {2007},
	note = {\_eprint: https://acsess.onlinelibrary.wiley.com/doi/pdf/10.2135/cropsci2006.11.0690},
	pages = {1082--1090},
	file = {Full Text PDF:C\:\\Users\\vamsi\\Zotero\\storage\\5RWBN3XF\\Bernardo and Yu - 2007 - Prospects for Genomewide Selection for Quantitativ.pdf:application/pdf;Snapshot:C\:\\Users\\vamsi\\Zotero\\storage\\9WW2QHV8\\cropsci2006.11.html:text/html}
}

@article{crossa_prediction_2010,
	title = {Prediction of {Genetic} {Values} of {Quantitative} {Traits} in {Plant} {Breeding} {Using} {Pedigree} and {Molecular} {Markers}},
	volume = {186},
	copyright = {Copyright © 2010 by the Genetics Society of America},
	issn = {0016-6731, 1943-2631},
	url = {https://www.genetics.org/content/186/2/713},
	doi = {10.1534/genetics.110.118521},
	abstract = {The availability of dense molecular markers has made possible the use of genomic selection (GS) for plant breeding. However, the evaluation of models for GS in real plant populations is very limited. This article evaluates the performance of parametric and semiparametric models for GS using wheat (Triticum aestivum L.) and maize (Zea mays) data in which different traits were measured in several environmental conditions. The findings, based on extensive cross-validations, indicate that models including marker information had higher predictive ability than pedigree-based models. In the wheat data set, and relative to a pedigree model, gains in predictive ability due to inclusion of markers ranged from 7.7 to 35.7\%. Correlation between observed and predictive values in the maize data set achieved values up to 0.79. Estimates of marker effects were different across environmental conditions, indicating that genotype × environment interaction is an important component of genetic variability. These results indicate that GS in plant breeding can be an effective strategy for selecting among lines whose phenotypes have yet to be observed.},
	language = {en},
	number = {2},
	urldate = {2020-12-17},
	journal = {Genetics},
	author = {Crossa, José and Campos, Gustavo de los and Pérez, Paulino and Gianola, Daniel and Burgueño, Juan and Araus, José Luis and Makumbi, Dan and Singh, Ravi P. and Dreisigacker, Susanne and Yan, Jianbing and Arief, Vivi and Banziger, Marianne and Braun, Hans-Joachim},
	month = oct,
	year = {2010},
	pmid = {20813882},
	note = {Publisher: Genetics
Section: Investigations},
	pages = {713--724},
	file = {Full Text PDF:C\:\\Users\\vamsi\\Zotero\\storage\\YAUDU25D\\Crossa et al. - 2010 - Prediction of Genetic Values of Quantitative Trait.pdf:application/pdf;Snapshot:C\:\\Users\\vamsi\\Zotero\\storage\\Q97EQNE5\\713.html:text/html}
}

@article{crossa_genomic_2011,
	title = {Genomic {Selection} and {Prediction} in {Plant} {Breeding}},
	volume = {25},
	issn = {1542-7528},
	url = {https://doi.org/10.1080/15427528.2011.558767},
	doi = {10.1080/15427528.2011.558767},
	abstract = {The availability of thousands of genome-wide molecular markers has made possible the use of genomic selection in plants and animals. However, the evaluation of models for genomic selection in plant breeding populations remains limited. In this study, we provide an overview of several models for genomic selection, whose predictive ability we investigate using two plant data sets. The first data set comprises historical phenotypic records of a series of wheat (Triticum aestivum L.) trials evaluated in 10 environments and recently generated genomic data. The second data set pertains to international maize (Zea mays L.) trials in which two disease traits (Exserohilum turcicum and Cercospora zeae-maydis) of maize lines evaluated in five environments were measured. Results showed that models including marker information yielded important gains in predictive ability relative to that of a pedigree-based model, this with a modest number of markers. Estimates of marker effects were different across environmental conditions, indicating that genotype × environment interaction was an important component of genetic variability. Overall, the study provided evidence from real populations indicating that genomic selection could be an effective tool for improving traits of economic importance in commercial crops.},
	number = {3},
	urldate = {2020-12-17},
	journal = {Journal of Crop Improvement},
	author = {Crossa, José and Pérez, Paulino and Campos, Gustavo de los and Mahuku, George and Dreisigacker, Susanne and Magorokosho, Cosmos},
	month = apr,
	year = {2011},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/15427528.2011.558767},
	keywords = {best linear unbiased predictors (BLUP), breeding values, genomic selection, parametric and non-parametric regression, prediction Bayesian estimates},
	pages = {239--261},
	file = {Full Text PDF:C\:\\Users\\vamsi\\Zotero\\storage\\VX4SHCCZ\\Crossa et al. - 2011 - Genomic Selection and Prediction in Plant Breeding.pdf:application/pdf;Snapshot:C\:\\Users\\vamsi\\Zotero\\storage\\UUW38PB4\\15427528.2011.html:text/html}
}

@article{usai_lasso_2009,
	title = {{LASSO} with cross-validation for genomic selection},
	abstract = {All in-text	references	underlined	in	blue	are	linked	to	publications	on	ResearchGate, letting you	access	and	read	them	immediately.},
	journal = {Genet. Res. (Camb). 91(6},
	author = {Usai, Mario Graziano and Sardegna, Agris and Profile, See and Usai, M. Graziano and Goddard, Mike E. and Hayes, Ben J.},
	year = {2009},
	pages = {436},
	file = {Citeseer - Full Text PDF:C\:\\Users\\vamsi\\Zotero\\storage\\E7PGISFQ\\Usai et al. - 2009 - LASSO with cross-validation for genomic selection.pdf:application/pdf;Citeseer - Snapshot:C\:\\Users\\vamsi\\Zotero\\storage\\IY8Q38YT\\download.html:text/html}
}

@article{gianola_genomic-assisted_2006,
	title = {Genomic-assisted prediction of genetic value with semiparametric procedures},
	volume = {173},
	issn = {0016-6731},
	doi = {10.1534/genetics.105.049510},
	abstract = {Semiparametric procedures for prediction of total genetic value for quantitative traits, which make use of phenotypic and genomic data simultaneously, are presented. The methods focus on the treatment of massive information provided by, e.g., single-nucleotide polymorphisms. It is argued that standard parametric methods for quantitative genetic analysis cannot handle the multiplicity of potential interactions arising in models with, e.g., hundreds of thousands of markers, and that most of the assumptions required for an orthogonal decomposition of variance are violated in artificial and natural populations. This makes nonparametric procedures attractive. Kernel regression and reproducing kernel Hilbert spaces regression procedures are embedded into standard mixed-effects linear models, retaining additive genetic effects under multivariate normality for operational reasons. Inferential procedures are presented, and some extensions are suggested. An example is presented, illustrating the potential of the methodology. Implementations can be carried out after modification of standard software developed by animal breeders for likelihood-based or Bayesian analysis.},
	language = {eng},
	number = {3},
	journal = {Genetics},
	author = {Gianola, Daniel and Fernando, Rohan L. and Stella, Alessandra},
	month = jul,
	year = {2006},
	pmid = {16648593},
	pmcid = {PMC1526664},
	keywords = {Algorithms, Genomics, Genotype, Linear Models, Phenotype, Polymorphism, Single Nucleotide, Quantitative Trait Loci, Statistics, Nonparametric},
	pages = {1761--1776},
	file = {Full Text:C\:\\Users\\vamsi\\Zotero\\storage\\F6YIEF2C\\Gianola et al. - 2006 - Genomic-assisted prediction of genetic value with .pdf:application/pdf}
}

@article{de_los_campos_semi-parametric_2010,
	title = {Semi-parametric genomic-enabled prediction of genetic values using reproducing kernel {Hilbert} spaces methods},
	volume = {92},
	issn = {1469-5073},
	doi = {10.1017/S0016672310000285},
	abstract = {Prediction of genetic values is a central problem in quantitative genetics. Over many decades, such predictions have been successfully accomplished using information on phenotypic records and family structure usually represented with a pedigree. Dense molecular markers are now available in the genome of humans, plants and animals, and this information can be used to enhance the prediction of genetic values. However, the incorporation of dense molecular marker data into models poses many statistical and computational challenges, such as how models can cope with the genetic complexity of multi-factorial traits and with the curse of dimensionality that arises when the number of markers exceeds the number of data points. Reproducing kernel Hilbert spaces regressions can be used to address some of these challenges. The methodology allows regressions on almost any type of prediction sets (covariates, graphs, strings, images, etc.) and has important computational advantages relative to many parametric approaches. Moreover, some parametric models appear as special cases. This article provides an overview of the methodology, a discussion of the problem of kernel choice with a focus on genetic applications, algorithms for kernel selection and an assessment of the proposed methods using a collection of 599 wheat lines evaluated for grain yield in four mega environments.},
	language = {eng},
	number = {4},
	journal = {Genetics Research},
	author = {De los Campos, Gustavo and Gianola, Daniel and Rosa, Guilherme J. M. and Weigel, Kent A. and Crossa, José},
	month = aug,
	year = {2010},
	pmid = {20943010},
	keywords = {Bayes Theorem, Breeding, Genetic Markers, Models, Genetic, Models, Statistical, Regression Analysis, Triticum},
	pages = {295--308},
	file = {Submitted Version:C\:\\Users\\vamsi\\Zotero\\storage\\6C5YPRPZ\\De los Campos et al. - 2010 - Semi-parametric genomic-enabled prediction of gene.pdf:application/pdf}
}

@article{long_application_2011,
	title = {Application of support vector regression to genome-assisted prediction of quantitative traits},
	volume = {123},
	issn = {1432-2242},
	url = {https://doi.org/10.1007/s00122-011-1648-y},
	doi = {10.1007/s00122-011-1648-y},
	abstract = {A byproduct of genome-wide association studies is the possibility of carrying out genome-enabled prediction of disease risk or of quantitative traits. This study is concerned with predicting two quantitative traits, milk yield in dairy cattle and grain yield in wheat, using dense molecular markers as predictors. Two support vector regression (SVR) models, ε-SVR and least-squares SVR, were explored and compared to a widely applied linear regression model, the Bayesian Lasso, the latter assuming additive marker effects. Predictive performance was measured using predictive correlation and mean squared error of prediction. Depending on the kernel function chosen, SVR can model either linear or nonlinear relationships between phenotypes and marker genotypes. For milk yield, where phenotypes were estimated breeding values of bulls (a linear combination of the data), SVR with a Gaussian radial basis function (RBF) kernel had a slightly better performance than with a linear kernel, and was similar to the Bayesian Lasso. For the wheat data, where phenotype was raw grain yield, the RBF kernel provided clear advantages over the linear kernel, e.g., a 17.5\% increase in correlation when using the ε-SVR. SVR with a RBF kernel also compared favorably to the Bayesian Lasso in this case. It is concluded that a nonlinear RBF kernel may be an optimal choice for SVR, especially when phenotypes to be predicted have a nonlinear dependency on genotypes, as it might have been the case in the wheat data.},
	language = {en},
	number = {7},
	urldate = {2020-12-17},
	journal = {Theoretical and Applied Genetics},
	author = {Long, Nanye and Gianola, Daniel and Rosa, Guilherme J. M. and Weigel, Kent A.},
	month = jul,
	year = {2011},
	pages = {1065},
	file = {Springer Full Text PDF:C\:\\Users\\vamsi\\Zotero\\storage\\KTC4K7CH\\Long et al. - 2011 - Application of support vector regression to genome.pdf:application/pdf}
}

@article{moser_comparison_2009,
	title = {A comparison of five methods to predict genomic breeding values of dairy bulls from genome-wide {SNP} markers},
	volume = {41},
	issn = {1297-9686},
	doi = {10.1186/1297-9686-41-56},
	abstract = {BACKGROUND: Genomic selection (GS) uses molecular breeding values (MBV) derived from dense markers across the entire genome for selection of young animals. The accuracy of MBV prediction is important for a successful application of GS. Recently, several methods have been proposed to estimate MBV. Initial simulation studies have shown that these methods can accurately predict MBV. In this study we compared the accuracies and possible bias of five different regression methods in an empirical application in dairy cattle.
METHODS: Genotypes of 7,372 SNP and highly accurate EBV of 1,945 dairy bulls were used to predict MBV for protein percentage (PPT) and a profit index (Australian Selection Index, ASI). Marker effects were estimated by least squares regression (FR-LS), Bayesian regression (Bayes-R), random regression best linear unbiased prediction (RR-BLUP), partial least squares regression (PLSR) and nonparametric support vector regression (SVR) in a training set of 1,239 bulls. Accuracy and bias of MBV prediction were calculated from cross-validation of the training set and tested against a test team of 706 young bulls.
RESULTS: For both traits, FR-LS using a subset of SNP was significantly less accurate than all other methods which used all SNP. Accuracies obtained by Bayes-R, RR-BLUP, PLSR and SVR were very similar for ASI (0.39-0.45) and for PPT (0.55-0.61). Overall, SVR gave the highest accuracy.All methods resulted in biased MBV predictions for ASI, for PPT only RR-BLUP and SVR predictions were unbiased. A significant decrease in accuracy of prediction of ASI was seen in young test cohorts of bulls compared to the accuracy derived from cross-validation of the training set. This reduction was not apparent for PPT. Combining MBV predictions with pedigree based predictions gave 1.05 - 1.34 times higher accuracies compared to predictions based on pedigree alone. Some methods have largely different computational requirements, with PLSR and RR-BLUP requiring the least computing time.
CONCLUSIONS: The four methods which use information from all SNP namely RR-BLUP, Bayes-R, PLSR and SVR generate similar accuracies of MBV prediction for genomic selection, and their use in the selection of immediate future generations in dairy cattle will be comparable. The use of FR-LS in genomic selection is not recommended.},
	language = {eng},
	journal = {Genetics, selection, evolution: GSE},
	author = {Moser, Gerhard and Tier, Bruce and Crump, Ron E. and Khatkar, Mehar S. and Raadsma, Herman W.},
	month = dec,
	year = {2009},
	pmid = {20043835},
	pmcid = {PMC2814805},
	keywords = {Animals, Bayes Theorem, Bias, Breeding, Cattle, Cohort Studies, Dairying, Genetic Markers, Genome, Genotype, Least-Squares Analysis, Male, Models, Genetic, Pedigree, Phenotype, Polymorphism, Single Nucleotide, Predictive Value of Tests, Quantitative Trait Loci, Regression Analysis, Selection, Genetic, Statistics, Nonparametric},
	pages = {56},
	file = {Full Text:C\:\\Users\\vamsi\\Zotero\\storage\\XBUVQCLF\\Moser et al. - 2009 - A comparison of five methods to predict genomic br.pdf:application/pdf}
}

@article{burgueno_genomic_2012,
	title = {Genomic {Prediction} of {Breeding} {Values} when {Modeling} {Genotype} × {Environment} {Interaction} using {Pedigree} and {Dense} {Molecular} {Markers}},
	volume = {52},
	copyright = {© 2012 The Authors.},
	issn = {1435-0653},
	url = {https://acsess.onlinelibrary.wiley.com/doi/abs/10.2135/cropsci2011.06.0299},
	doi = {https://doi.org/10.2135/cropsci2011.06.0299},
	abstract = {Genomic selection (GS) has become an important aid in plant and animal breeding. Multienvironment (multitrait) models allow borrowing of information across environments (traits), which could enhance prediction accuracy. This study presents multienvironment (multitrait) models for GS and compares the predictive accuracy of these models with: (i) multienvironment analysis without pedigree and marker information, and (ii) multienvironment pedigree or/and marker-based models. A statistical framework for incorporating pedigree and molecular marker information in models for multienvironment data is described and applied to data that originate from wheat (Triticum aestivum L.) multienvironment trials. Two prediction problems relevant to plant breeders are considered: (CV1) predicting the performance of untested genotypes (“newly” developed lines), and (CV2) predicting the performance of genotypes that have been evaluated in some environments but not in others. Results confirmed the superiority of models using both marker and pedigree information over those based on pedigree information only. Models with pedigree and/or markers had better predictive accuracy than simple linear mixed models that do not include either of these two sources of information. We concluded that the evaluation of such trials can benefit greatly from using multienvironment GS models.},
	language = {en},
	number = {2},
	urldate = {2020-12-17},
	journal = {Crop Science},
	author = {Burgueño, Juan and Campos, Gustavo de los and Weigel, Kent and Crossa, José},
	year = {2012},
	note = {\_eprint: https://acsess.onlinelibrary.wiley.com/doi/pdf/10.2135/cropsci2011.06.0299},
	pages = {707--719},
	file = {Full Text PDF:C\:\\Users\\vamsi\\Zotero\\storage\\ZVF4ULQT\\Burgueño et al. - 2012 - Genomic Prediction of Breeding Values when Modelin.pdf:application/pdf}
}

@article{achlioptas_fast_2007,
	title = {Fast computation of low-rank matrix approximations},
	volume = {54},
	issn = {0004-5411, 1557-735X},
	url = {https://dl.acm.org/doi/10.1145/1219092.1219097},
	doi = {10.1145/1219092.1219097},
	abstract = {Given a matrix A, it is often desirable to ﬁnd a good approximation to A that has low rank. We introduce a simple technique for accelerating the computation of such approximations when A has strong spectral features, that is, when the singular values of interest are signiﬁcantly greater than those of a random matrix with size and entries similar to A. Our technique amounts to independently sampling and/or quantizing the entries of A, thus speeding up computation by reducing the number of nonzero entries and/or the length of their representation. Our analysis is based on observing that the acts of sampling and quantization can be viewed as adding a random matrix N to A, whose entries are independent random variables with zero-mean and bounded variance. Since, with high probability, N has very weak spectral features, we can prove that the effect of sampling and quantization nearly vanishes when a low-rank approximation to A + N is computed. We give high probability bounds on the quality of our approximation both in the Frobenius and the 2-norm.},
	language = {en},
	number = {2},
	urldate = {2020-12-17},
	journal = {Journal of the ACM},
	author = {Achlioptas, Dimitris and Mcsherry, Frank},
	month = apr,
	year = {2007},
	pages = {9},
	file = {Achlioptas and Mcsherry - 2007 - Fast computation of low-rank matrix approximations.pdf:C\:\\Users\\vamsi\\Zotero\\storage\\V6A7UD38\\Achlioptas and Mcsherry - 2007 - Fast computation of low-rank matrix approximations.pdf:application/pdf}
}

@article{drineas_note_2011,
	title = {A note on element-wise matrix sparsification via a matrix-valued {Bernstein} inequality},
	volume = {111},
	issn = {0020-0190},
	url = {http://www.sciencedirect.com/science/article/pii/S0020019011000196},
	doi = {10.1016/j.ipl.2011.01.010},
	abstract = {Given a matrix A∈Rn×n, we present a simple, element-wise sparsification algorithm that zeroes out all sufficiently small elements of A and then retains some of the remaining elements with probabilities proportional to the square of their magnitudes. We analyze the approximation accuracy of the proposed algorithm using a recent, elegant non-commutative Bernstein inequality, and compare our bounds with all existing (to the best of our knowledge) element-wise matrix sparsification algorithms.},
	language = {en},
	number = {8},
	urldate = {2020-12-17},
	journal = {Information Processing Letters},
	author = {Drineas, Petros and Zouzias, Anastasios},
	month = mar,
	year = {2011},
	keywords = {Algorithms, Matrix sparsification, Matrix-valued Bernstein bounds},
	pages = {385--389},
	file = {Submitted Version:C\:\\Users\\vamsi\\Zotero\\storage\\5VU4XFAY\\Drineas and Zouzias - 2011 - A note on element-wise matrix sparsification via a.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\vamsi\\Zotero\\storage\\H8A2S8NK\\S0020019011000196.html:text/html}
}

@article{fisher_use_1936,
	title = {The {Use} of {Multiple} {Measurements} in {Taxonomic} {Problems}},
	volume = {7},
	copyright = {1936 Blackwell Publishing Ltd/University College London},
	issn = {2050-1439},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-1809.1936.tb02137.x},
	doi = {https://doi.org/10.1111/j.1469-1809.1936.tb02137.x},
	abstract = {The articles published by the Annals of Eugenics (1925–1954) have been made available online as an historical archive intended for scholarly use. The work of eugenicists was often pervaded by prejudice against racial, ethnic and disabled groups. The online publication of this material for scholarly research purposes is not an endorsement of those views nor a promotion of eugenics in any way.},
	language = {en},
	number = {2},
	urldate = {2020-12-17},
	journal = {Annals of Eugenics},
	author = {Fisher, R. A.},
	year = {1936},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-1809.1936.tb02137.x},
	pages = {179--188},
	file = {Full Text PDF:C\:\\Users\\vamsi\\Zotero\\storage\\MAARF8EW\\Fisher - 1936 - The Use of Multiple Measurements in Taxonomic Prob.pdf:application/pdf}
}

@book{james_introduction_2013,
	address = {New York, NY},
	series = {Springer {Texts} in {Statistics}},
	title = {An {Introduction} to {Statistical} {Learning}},
	volume = {103},
	isbn = {978-1-4614-7137-0 978-1-4614-7138-7},
	url = {http://link.springer.com/10.1007/978-1-4614-7138-7},
	language = {en},
	urldate = {2020-12-17},
	publisher = {Springer New York},
	author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
	year = {2013},
	doi = {10.1007/978-1-4614-7138-7},
	file = {James et al. - 2013 - An Introduction to Statistical Learning.pdf:C\:\\Users\\vamsi\\Zotero\\storage\\NHNZUKJX\\James et al. - 2013 - An Introduction to Statistical Learning.pdf:application/pdf}
}

@article{deshpande_matrix_2006,
	title = {Matrix {Approximation} and {Projective} {Clustering} via {Volume} {Sampling}},
	volume = {2},
	issn = {1557-2862},
	url = {http://www.theoryofcomputing.org/articles/v002a012},
	doi = {10.4086/toc.2006.v002a012},
	language = {EN},
	number = {1},
	urldate = {2020-12-17},
	journal = {Theory of Computing},
	author = {Deshpande, Amit and Rademacher, Luis and Vempala, Santosh and Wang, Grant},
	month = oct,
	year = {2006},
	note = {Publisher: Theory of Computing Exchange},
	pages = {225--247},
	file = {Full Text PDF:C\:\\Users\\vamsi\\Zotero\\storage\\IXJZKRZ4\\Deshpande et al. - 2006 - Matrix Approximation and Projective Clustering via.pdf:application/pdf;Snapshot:C\:\\Users\\vamsi\\Zotero\\storage\\4WWAE3SX\\v002a012.html:text/html}
}

@article{drineas_fast_2012,
	title = {Fast approximation of matrix coherence and statistical leverage},
	volume = {13},
	issn = {1532-4435},
	abstract = {The statistical leverage scores of a matrix A are the squared row-norms of the matrix containing its (top) left singular vectors and the coherence is the largest leverage score. These quantities are of interest in recently-popular problems such as matrix completion and Nyström-based low-rank matrix approximation as well as in large-scale statistical data analysis applications more generally; moreover, they are of interest since they define the key structural nonuniformity that must be dealt with in developing fast randomized matrix algorithms. Our main result is a randomized algorithm that takes as input an arbitrary n × d matrix A, with n ≫ d, and that returns as output relative-error approximations to all n of the statistical leverage scores. The proposed algorithm runs (under assumptions on the precise values of n and d) in O(nd logn) time, as opposed to the O(nd2) time required by the naïve algorithm that involves computing an orthogonal basis for the range of A. Our analysis may be viewed in terms of computing a relative-error approximation to an underconstrained least-squares approximation problem, or, relatedly, it may be viewed as an application of Johnson-Lindenstrauss type ideas. Several practically-important extensions of our basic result are also described, including the approximation of so-called cross-leverage scores, the extension of these ideas to matrices with n ≈ d, and the extension to streaming environments.},
	number = {1},
	journal = {The Journal of Machine Learning Research},
	author = {Drineas, Petros and Magdon-Ismail, Malik and Mahoney, Michael W. and Woodruff, David P.},
	month = dec,
	year = {2012},
	keywords = {matrix coherence, randomized algorithm, statistical leverage},
	pages = {3475--3506},
	file = {Full Text PDF:C\:\\Users\\vamsi\\Zotero\\storage\\K5YX9SJS\\Drineas et al. - 2012 - Fast approximation of matrix coherence and statist.pdf:application/pdf}
}

@inproceedings{drineas_sampling_2006,
	address = {USA},
	series = {{SODA} '06},
	title = {Sampling algorithms for \textit{l}$_{\textrm{2}}$ regression and applications},
	isbn = {978-0-89871-605-4},
	abstract = {We present and analyze a sampling algorithm for the basic linear-algebraic problem of l2 regression. The l2 regression (or least-squares fit) problem takes as input a matrix A ∈ Rn×d (where we assume n {\textgreater} d) and a target vector b ∈ Rn, and it returns as output Z = minx∈Rd {\textbar}b - Ax{\textbar}2. Also of interest is xopt = A+b, where A+ is the Moore-Penrose generalized inverse, which is the minimum-length vector achieving the minimum. Our algorithm randomly samples r rows from the matrix A and vector b to construct an induced l2 regression problem with many fewer rows, but with the same number of columns. A crucial feature of the algorithm is the nonuniform sampling probabilities. These probabilities depend in a sophisticated manner on the lengths, i.e., the Euclidean norms, of the rows of the left singular vectors of A and the manner in which b lies in the complement of the column space of A. Under appropriate assumptions, we show relative error approximations for both Z and xopt. Applications of this sampling methodology are briefly discussed.},
	urldate = {2020-12-16},
	booktitle = {Proceedings of the seventeenth annual {ACM}-{SIAM} symposium on {Discrete} algorithm},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Drineas, Petros and Mahoney, Michael W. and Muthukrishnan, S.},
	month = jan,
	year = {2006},
	pages = {1127--1136},
	file = {Full Text PDF:C\:\\Users\\vamsi\\Zotero\\storage\\DLQC2L4V\\Drineas et al. - 2006 - Sampling algorithms for l2 regre.pdf:application/pdf}
}

@article{drineas_relative-error_2008,
	title = {Relative-{Error} \${CUR}\$ {Matrix} {Decompositions}},
	volume = {30},
	issn = {0895-4798},
	url = {https://doi.org/10.1137/07070471X},
	doi = {10.1137/07070471X},
	abstract = {Many data analysis applications deal with large matrices and involve approximating the matrix using a small number of “components.” Typically, these components are linear combinations of the rows and columns of the matrix, and are thus difficult to interpret in terms of the original features of the input data. In this paper, we propose and study matrix approximations that are explicitly expressed in terms of a small number of columns and/or rows of the data matrix, and thereby more amenable to interpretation in terms of the original data. Our main algorithmic results are two randomized algorithms which take as input an \$m{\textbackslash}times n\$ matrix \$A\$ and a rank parameter \$k\$. In our first algorithm, \$C\$ is chosen, and we let \$A'=CC{\textasciicircum}+A\$, where \$C{\textasciicircum}+\$ is the Moore-Penrose generalized inverse of \$C\$. In our second algorithm \$C\$, \$U\$, \$R\$ are chosen, and we let \$A'=CUR\$. (\$C\$ and \$R\$ are matrices that consist of actual columns and rows, respectively, of \$A\$, and \$U\$ is a generalized inverse of their intersection.) For each algorithm, we show that with probability at least \$1-{\textbackslash}delta\$, \${\textbackslash}{\textbar}A-A'{\textbackslash}{\textbar}\_F{\textbackslash}leq(1+{\textbackslash}epsilon){\textbackslash},{\textbackslash}{\textbar}A-A\_k{\textbackslash}{\textbar}\_F\$, where \$A\_k\$ is the “best” rank-\$k\$ approximation provided by truncating the SVD of \$A\$, and where \${\textbackslash}{\textbar}X{\textbackslash}{\textbar}\_F\$ is the Frobenius norm of the matrix \$X\$. The number of columns of \$C\$ and rows of \$R\$ is a low-degree polynomial in \$k\$, \$1/{\textbackslash}epsilon\$, and \${\textbackslash}log(1/{\textbackslash}delta)\$. Both the Numerical Linear Algebra community and the Theoretical Computer Science community have studied variants of these matrix decompositions over the last ten years. However, our two algorithms are the first polynomial time algorithms for such low-rank matrix approximations that come with relative-error guarantees; previously, in some cases, it was not even known whether such matrix decompositions exist. Both of our algorithms are simple and they take time of the order needed to approximately compute the top \$k\$ singular vectors of \$A\$. The technical crux of our analysis is a novel, intuitive sampling method we introduce in this paper called “subspace sampling.” In subspace sampling, the sampling probabilities depend on the Euclidean norms of the rows of the top singular vectors. This allows us to obtain provable relative-error guarantees by deconvoluting “subspace” information and “size-of-\$A\$” information in the input matrix. This technique is likely to be useful for other matrix approximation and data analysis problems.},
	number = {2},
	urldate = {2020-12-17},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	author = {Drineas, Petros and Mahoney, Michael W. and Muthukrishnan, S.},
	month = sep,
	year = {2008},
	keywords = {\$CUR\$ matrix decomposition, approximate least squares, data analysis, random sampling algorithms},
	pages = {844--881},
	file = {Submitted Version:C\:\\Users\\vamsi\\Zotero\\storage\\9G3C8KJB\\Drineas et al. - 2008 - Relative-Error \$CUR\$ Matrix Decompositions.pdf:application/pdf}
}

@article{clarkson_low-rank_2017,
	title = {Low-{Rank} {Approximation} and {Regression} in {Input} {Sparsity} {Time}},
	volume = {63},
	issn = {0004-5411},
	url = {https://doi.org/10.1145/3019134},
	doi = {10.1145/3019134},
	abstract = {We design a new distribution over m × n matrices S so that, for any fixed n × d matrix A of rank r, with probability at least 9/10, ∥SAx∥2 = (1 ± ε)∥Ax∥2 simultaneously for all x ∈ Rd. Here, m is bounded by a polynomial in rε− 1, and the parameter ε ∈ (0, 1]. Such a matrix S is called a subspace embedding. Furthermore, SA can be computed in O(nnz(A)) time, where nnz(A) is the number of nonzero entries of A. This improves over all previous subspace embeddings, for which computing SA required at least Ω(ndlog d) time. We call these S sparse embedding matrices. Using our sparse embedding matrices, we obtain the fastest known algorithms for overconstrained least-squares regression, low-rank approximation, approximating all leverage scores, and ℓp regression. More specifically, let b be an n × 1 vector, ε {\textgreater} 0 a small enough value, and integers k, p ⩾ 1. Our results include the following. —Regression: The regression problem is to find d × 1 vector x′ for which ∥Ax′ − b∥p ⩽ (1 + ε)min x∥Ax − b∥p. For the Euclidean case p = 2, we obtain an algorithm running in O(nnz(A)) + Õ(d3ε −2) time, and another in O(nnz(A)log(1/ε)) + Õ(d3 log (1/ε)) time. (Here, Õ(f) = f ċ log O(1)(f).) For p ∈ [1, ∞), more generally, we obtain an algorithm running in O(nnz(A) log n) + O(r{\textbackslash}ε −1)C time, for a fixed C. —Low-rank approximation: We give an algorithm to obtain a rank-k matrix Âk such that ∥A − Âk∥F ≤ (1 + ε )∥ A − Ak∥F, where Ak is the best rank-k approximation to A. (That is, Ak is the output of principal components analysis, produced by a truncated singular value decomposition, useful for latent semantic indexing and many other statistical problems.) Our algorithm runs in O(nnz(A)) + Õ(nk2ε−4 + k3ε−5) time. —Leverage scores: We give an algorithm to estimate the leverage scores of A, up to a constant factor, in O(nnz(A)log n) + Õ(r3)time.},
	number = {6},
	urldate = {2020-12-17},
	journal = {Journal of the ACM},
	author = {Clarkson, Kenneth L. and Woodruff, David P.},
	month = jan,
	year = {2017},
	keywords = {approximation, Matrices, randomized},
	pages = {54:1--54:45},
	file = {Full Text PDF:C\:\\Users\\vamsi\\Zotero\\storage\\KZQKGCDE\\Clarkson and Woodruff - 2017 - Low-Rank Approximation and Regression in Input Spa.pdf:application/pdf}
}

@article{frieze_fast_2004,
	title = {Fast monte-carlo algorithms for finding low-rank approximations},
	volume = {51},
	issn = {0004-5411},
	url = {https://doi.org/10.1145/1039488.1039494},
	doi = {10.1145/1039488.1039494},
	abstract = {We consider the problem of approximating a given m × n matrix A by another matrix of specified rank k, which is smaller than m and n. The Singular Value Decomposition (SVD) can be used to find the "best" such approximation. However, it takes time polynomial in m, n which is prohibitive for some modern applications. In this article, we develop an algorithm that is qualitatively faster, provided we may sample the entries of the matrix in accordance with a natural probability distribution. In many applications, such sampling can be done efficiently. Our main result is a randomized algorithm to find the description of a matrix D* of rank at most k so that holds with probability at least 1 − δ (where {\textbar}·{\textbar}F is the Frobenius norm). The algorithm takes time polynomial in k,1/ϵ, log(1/δ) only and is independent of m and n. In particular, this implies that in constant time, it can be determined if a given matrix of arbitrary size has a good low-rank approximation.},
	number = {6},
	urldate = {2020-12-17},
	journal = {Journal of the ACM},
	author = {Frieze, Alan and Kannan, Ravi and Vempala, Santosh},
	month = nov,
	year = {2004},
	keywords = {low-rank approximation, Matrix algorithms, sampling},
	pages = {1025--1041},
	file = {Full Text PDF:C\:\\Users\\vamsi\\Zotero\\storage\\5EGB4P6V\\Frieze et al. - 2004 - Fast monte-carlo algorithms for finding low-rank a.pdf:application/pdf}
}

@article{tropp_practical_2017,
	title = {Practical {Sketching} {Algorithms} for {Low}-{Rank} {Matrix} {Approximation}},
	volume = {38},
	issn = {0895-4798},
	url = {https://epubs.siam.org/doi/10.1137/17M1111590},
	doi = {10.1137/17M1111590},
	abstract = {This paper describes a suite of algorithms for constructing low-rank approximations of an input matrix from a random linear image, or sketch, of the matrix. These methods can preserve structural properties of the input matrix, such as positive-semidefiniteness, and they can produce approximations with a user-specified rank. The algorithms are simple, accurate, numerically stable, and provably correct. Moreover, each method is accompanied by an informative error bound that allows users to select parameters a priori to achieve a given approximation quality. These claims are supported by numerical experiments with real and synthetic data.},
	number = {4},
	urldate = {2020-12-17},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	author = {Tropp, Joel A. and Yurtsever, Alp and Udell, Madeleine and Cevher, Volkan},
	month = jan,
	year = {2017},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {1454--1485},
	file = {Full Text PDF:C\:\\Users\\vamsi\\Zotero\\storage\\JJ4B7K6J\\Tropp et al. - 2017 - Practical Sketching Algorithms for Low-Rank Matrix.pdf:application/pdf;Snapshot:C\:\\Users\\vamsi\\Zotero\\storage\\IFUMQ2IF\\17M1111590.html:text/html}
}

@misc{noauthor_regression_nodate,
	title = {Regression {Shrinkage} and {Selection} {Via} the {Lasso} - {Tibshirani} - 1996 - {Journal} of the {Royal} {Statistical} {Society}: {Series} {B} ({Methodological}) - {Wiley} {Online} {Library}},
	url = {https://rss.onlinelibrary.wiley.com/doi/10.1111/j.2517-6161.1996.tb02080.x},
	urldate = {2020-12-17},
	file = {Regression Shrinkage and Selection Via the Lasso - Tibshirani - 1996 - Journal of the Royal Statistical Society\: Series B (Methodological) - Wiley Online Library:C\:\\Users\\vamsi\\Zotero\\storage\\2NRVLDIF\\j.2517-6161.1996.tb02080.html:text/html}
}

@article{sneath_numerical_1973,
	title = {Numerical taxonomy. {The} principles and practice of numerical classification.},
	url = {https://www.cabdirect.org/cabdirect/abstract/19730310919},
	abstract = {On a chart showing the development of numerical taxonomy in the first chapter of this book the authors characterize 1963 as the year of publication of their own "Principles of Numerical Taxonomy" [HcA 35, 280]. This was indeed a pioneering work on the subject, and the present volume is an extensively revised, updated and expanded version of it, sufficient to justify a change in title and in the...},
	language = {English},
	urldate = {2020-12-17},
	journal = {Numerical taxonomy. The principles and practice of numerical classification.},
	author = {Sneath, P. H. A. and Sokal, R. R.},
	year = {1973},
	file = {Snapshot:C\:\\Users\\vamsi\\Zotero\\storage\\IDTP6YLM\\19730310919.html:text/html}
}

@misc{noauthor_sensitivity_nodate,
	title = {Sensitivity {Analysis} in {Linear} {Regression} {\textbar} {Wiley} {Series} in {Probability} and {Statistics}},
	url = {https://onlinelibrary.wiley.com/doi/book/10.1002/9780470316764},
	urldate = {2020-12-17},
	file = {Sensitivity Analysis in Linear Regression | Wiley Series in Probability and Statistics:C\:\\Users\\vamsi\\Zotero\\storage\\UTFZKLSY\\9780470316764.html:text/html}
}

@article{velleman_efficient_1981,
	title = {Efficient {Computing} of {Regression} {Diagnostics}},
	volume = {35},
	issn = {0003-1305},
	url = {https://www.tandfonline.com/doi/abs/10.1080/00031305.1981.10479362},
	doi = {10.1080/00031305.1981.10479362},
	abstract = {Multiple regression diagnostic methods have recently been developed to help data analysts identify failures of data to adhere to the assumptions that customarily accompany regression models. However, the mathematical development of regression diagnostics has not generally led to efficient computing formulas. Conflicting terminology and the use of closely related but subtly different statistics has caused confusion. This article attempts to make regression diagnostics more readily available to those who compute regressions with packaged statistics programs. We review regression diagnostic methodology, highlighting ambiguities of terminology and relationships among similar methods. We present new formulas for efficient computing of regression diagnostics. Finally, we offer specific advice on obtaining regression diagnostics from existing statistics programs, with examples drawn from Minitab and SAS.},
	number = {4},
	urldate = {2020-12-17},
	journal = {The American Statistician},
	author = {Velleman, Paul F. and Welsch, Roy E.},
	month = nov,
	year = {1981},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/00031305.1981.10479362},
	keywords = {Collinearity, Influential data, Leverage, Minitab, Multicollinearity, Residuals, SAS},
	pages = {234--242},
	file = {Snapshot:C\:\\Users\\vamsi\\Zotero\\storage\\PDWI47TS\\00031305.1981.html:text/html}
}

@article{hoaglin_hat_1978,
	title = {The {Hat} {Matrix} in {Regression} and {ANOVA}},
	volume = {32},
	issn = {0003-1305},
	url = {https://www.tandfonline.com/doi/abs/10.1080/00031305.1978.10479237},
	doi = {10.1080/00031305.1978.10479237},
	abstract = {In least-squares fitting it is important to understand the influence which a data y value will have on each fitted y value. A projection matrix known as the hat matrix contains this information and, together with the Studentized residuals, provides a means of identifying exceptional data points. This approach also simplifies the calculations involved in removing a data point, and it requires only simple modifications in the preferred numerical least-squares algorithms.},
	number = {1},
	urldate = {2020-12-17},
	journal = {The American Statistician},
	author = {Hoaglin, David C. and Welsch, Roy E.},
	month = feb,
	year = {1978},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/00031305.1978.10479237},
	keywords = {Analysis of variance, Least-squares computations, Outliers, Projection matrix, Regression analysis, Studentized residuals},
	pages = {17--22},
	file = {Submitted Version:C\:\\Users\\vamsi\\Zotero\\storage\\K849R4M9\\Hoaglin and Welsch - 1978 - The Hat Matrix in Regression and ANOVA.pdf:application/pdf;Snapshot:C\:\\Users\\vamsi\\Zotero\\storage\\6ZGGTECS\\00031305.1978.html:text/html}
}

@article{fedoruk_dimensionality_2018,
	title = {Dimensionality reduction via the {Johnson}–{Lindenstrauss} {Lemma}: theoretical and empirical bounds on embedding dimension},
	volume = {74},
	issn = {1573-0484},
	shorttitle = {Dimensionality reduction via the {Johnson}–{Lindenstrauss} {Lemma}},
	url = {https://doi.org/10.1007/s11227-018-2401-y},
	doi = {10.1007/s11227-018-2401-y},
	abstract = {The Johnson–Lindenstrauss (JL) lemma has led to the development of tools for dealing with datasets in high dimensions. The lemma asserts that a set of high-dimensional points can be projected into lower dimensions, while approximately preserving the pairwise distance structure. Significant improvements of the JL lemma since its inception are summarized. Particular focus is placed on reproving Matoušek’s versions of the lemma (Random Struct Algorithms 33(2):142–156, 2008) first using subgaussian projection coefficients and then using sparse projection coefficients. The results of the lemma are illustrated using simulated data. The simulation suggests a projection that is more effective in terms of dimensionality reduction than is borne out by the theory. This more effective projection was applied to a very large natural, rather than simulated, dataset thus further strengthening empirical evidence of the existence of a better than the proven optimal lower bound on the embedding dimension. Additionally, we provide comparisons with other commonly used data reduction and simplification techniques.},
	language = {en},
	number = {8},
	urldate = {2020-12-17},
	journal = {The Journal of Supercomputing},
	author = {Fedoruk, John and Schmuland, Byron and Johnson, Julia and Heo, Giseon},
	month = aug,
	year = {2018},
	pages = {3933--3949},
	file = {Springer Full Text PDF:C\:\\Users\\vamsi\\Zotero\\storage\\DBT2XWQ4\\Fedoruk et al. - 2018 - Dimensionality reduction via the Johnson–Lindenstr.pdf:application/pdf}
}

@misc{noauthor_johnson-lindenstrauss_nodate,
	title = {The {Johnson}-{Lindenstrauss} {Lemma} and the sphericity of some graphs {\textbar} {Journal} of {Combinatorial} {Theory} {Series} {A}},
	url = {https://dl.acm.org/doi/10.5555/48184.48193},
	urldate = {2020-12-17},
	file = {The Johnson-Lindenstrauss Lemma and the sphericity of some graphs | Journal of Combinatorial Theory Series A:C\:\\Users\\vamsi\\Zotero\\storage\\9AGN5FER\\48184.html:text/html}
}

@misc{noauthor_pii_nodate,
	title = {{PII}: 0095-8956(88)90043-3 {\textbar} {Elsevier} {Enhanced} {Reader}},
	shorttitle = {{PII}},
	url = {https://reader.elsevier.com/reader/sd/pii/0095895688900433?token=C08433E75C41DAE3BD3EBAAB2BC1FA74EA214BBA78366CBDE19F74220E0B3745A73E6426428E0125AEBEFEB1E731106B},
	language = {en},
	urldate = {2020-12-17},
	doi = {10.1016/0095-8956(88)90043-3},
	note = {ISSN: 0095-8956},
	file = {Snapshot:C\:\\Users\\vamsi\\Zotero\\storage\\UGLEX8T4\\0095895688900433.html:text/html}
}

@article{dasgupta_elementary_2003,
	title = {An elementary proof of a theorem of {Johnson} and {Lindenstrauss}},
	volume = {22},
	issn = {1098-2418},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/rsa.10073},
	doi = {https://doi.org/10.1002/rsa.10073},
	abstract = {A result of Johnson and Lindenstrauss [13] shows that a set of n points in high dimensional Euclidean space can be mapped into an O(log n/ϵ2)-dimensional Euclidean space such that the distance between any two points changes by only a factor of (1 ± ϵ). In this note, we prove this theorem using elementary probabilistic techniques. © 2002 Wiley Periodicals, Inc. Random Struct. Alg., 22: 60–65, 2002},
	language = {en},
	number = {1},
	urldate = {2020-12-17},
	journal = {Random Structures \& Algorithms},
	author = {Dasgupta, Sanjoy and Gupta, Anupam},
	year = {2003},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/rsa.10073},
	pages = {60--65},
	file = {Snapshot:C\:\\Users\\vamsi\\Zotero\\storage\\DRLGNIQ4\\rsa.html:text/html;Full Text PDF:C\:\\Users\\vamsi\\Zotero\\storage\\33ML3ZCJ\\Dasgupta and Gupta - 2003 - An elementary proof of a theorem of Johnson and Li.pdf:application/pdf}
}

@inproceedings{indyk_approximate_1998,
	address = {Dallas, Texas, United States},
	title = {Approximate nearest neighbors: towards removing the curse of dimensionality},
	isbn = {978-0-89791-962-3},
	shorttitle = {Approximate nearest neighbors},
	url = {http://portal.acm.org/citation.cfm?doid=276698.276876},
	doi = {10.1145/276698.276876},
	abstract = {The nearest neighbor problem is the follolving: Given a set of n points P = (PI, . . . ,p,\vphantom{\{}\} in some metric space X, preprocess P so as to efficiently answer queries which require finding bhe point in P closest to a query point q E X. We focus on the particularly interesting case of the d-dimensional Euclidean space where X = Wd under some Zp norm. Despite decades of effort, t,he current solutions are far from saabisfactory; in fact, for large d, in theory or in practice, they provide litt,le improvement over the brute-force algorithm which compares the query point to each data point. Of late, t,here has been some interest in the approximate newest neighbors problem, which is: Find a point p E P that is an c-approximate nearest neighbor of the query q in t,hat for all p’ E P, d(p, q) {\textless} (1 + e)d(p’, q).},
	language = {en},
	urldate = {2020-12-17},
	booktitle = {Proceedings of the thirtieth annual {ACM} symposium on {Theory} of computing  - {STOC} '98},
	publisher = {ACM Press},
	author = {Indyk, Piotr and Motwani, Rajeev},
	year = {1998},
	pages = {604--613},
	file = {Indyk and Motwani - 1998 - Approximate nearest neighbors towards removing th.pdf:C\:\\Users\\vamsi\\Zotero\\storage\\UVQJJJTC\\Indyk and Motwani - 1998 - Approximate nearest neighbors towards removing th.pdf:application/pdf}
}

@article{har-peled_approximate_2012,
	title = {Approximate {Nearest} {Neighbor}: {Towards} {Removing} the {Curse} of {Dimensionality}},
	volume = {8},
	issn = {1557-2862},
	shorttitle = {Approximate {Nearest} {Neighbor}},
	url = {http://www.theoryofcomputing.org/articles/v008a014},
	doi = {10.4086/toc.2012.v008a014},
	language = {EN},
	number = {1},
	urldate = {2020-12-17},
	journal = {Theory of Computing},
	author = {Har-Peled, Sariel and Indyk, Piotr and Motwani, Rajeev},
	month = jul,
	year = {2012},
	note = {Publisher: Theory of Computing Exchange},
	pages = {321--350},
	file = {Full Text PDF:C\:\\Users\\vamsi\\Zotero\\storage\\3CCJPXT2\\Har-Peled et al. - 2012 - Approximate Nearest Neighbor Towards Removing the.pdf:application/pdf;Snapshot:C\:\\Users\\vamsi\\Zotero\\storage\\4NMBE59M\\v008a014.html:text/html}
}

@article{tropp_improved_2011,
	title = {Improved analysis of the subsampled randomized {Hadamard} transform},
	url = {http://arxiv.org/abs/1011.1595},
	abstract = {This paper presents an improved analysis of a structured dimension-reduction map called the subsampled randomized Hadamard transform. This argument demonstrates that the map preserves the Euclidean geometry of an entire subspace of vectors. The new proof is much simpler than previous approaches, and it offers---for the first time---optimal constants in the estimate on the number of dimensions required for the embedding.},
	urldate = {2020-12-17},
	journal = {arXiv:1011.1595 [cs, math]},
	author = {Tropp, Joel A.},
	month = jul,
	year = {2011},
	note = {arXiv: 1011.1595},
	keywords = {15B52, Computer Science - Data Structures and Algorithms, Mathematics - Numerical Analysis, Mathematics - Probability},
	annote = {Comment: 8 pages. To appear, Advances in Adaptive Data Analysis, special issue "Sparse Representation of Data and Images." v2--v4 include minor corrections},
	file = {arXiv Fulltext PDF:C\:\\Users\\vamsi\\Zotero\\storage\\CUIM6BA8\\Tropp - 2011 - Improved analysis of the subsampled randomized Had.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\vamsi\\Zotero\\storage\\7IMTXUHX\\1011.html:text/html}
}

@article{ahfock_statistical_2019,
	title = {Statistical properties of sketching algorithms},
	url = {http://arxiv.org/abs/1706.03665},
	abstract = {Sketching is a probabilistic data compression technique that has been largely developed in the computer science community. Numerical operations on big datasets can be intolerably slow; sketching algorithms address this issue by generating a smaller surrogate dataset. Typically, inference proceeds on the compressed dataset. Sketching algorithms generally use random projections to compress the original dataset and this stochastic generation process makes them amenable to statistical analysis. We argue that the sketched data can be modelled as a random sample, thus placing this family of data compression methods firmly within an inferential framework. In particular, we focus on the Gaussian, Hadamard and Clarkson-Woodruff sketches, and their use in single pass sketching algorithms for linear regression with huge \$n\$. We explore the statistical properties of sketched regression algorithms and derive new distributional results for a large class of sketched estimators. A key result is a conditional central limit theorem for data oblivious sketches. An important finding is that the best choice of sketching algorithm in terms of mean square error is related to the signal to noise ratio in the source dataset. Finally, we demonstrate the theory and the limits of its applicability on two real datasets.},
	urldate = {2020-12-17},
	journal = {arXiv:1706.03665 [stat]},
	author = {Ahfock, Daniel and Astle, William J. and Richardson, Sylvia},
	month = apr,
	year = {2019},
	note = {arXiv: 1706.03665},
	keywords = {Statistics - Computation, Statistics - Methodology},
	annote = {Comment: added central limit theorem under weaker conditions, unconditional results, corrected expression for variance of partial sketch},
	file = {arXiv Fulltext PDF:C\:\\Users\\vamsi\\Zotero\\storage\\RX74WXS6\\Ahfock et al. - 2019 - Statistical properties of sketching algorithms.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\vamsi\\Zotero\\storage\\RTEW4YRZ\\1706.html:text/html}
}

@article{ma_statistical_2015,
	title = {A {Statistical} {Perspective} on {Algorithmic} {Leveraging}},
	volume = {16},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v16/ma15a.html},
	number = {27},
	urldate = {2020-12-17},
	journal = {Journal of Machine Learning Research},
	author = {Ma, Ping and Mahoney, Michael W. and Yu, Bin},
	year = {2015},
	pages = {861--911},
	file = {Full Text PDF:C\:\\Users\\vamsi\\Zotero\\storage\\YQEKPJ6G\\Ma et al. - 2015 - A Statistical Perspective on Algorithmic Leveragin.pdf:application/pdf;Snapshot:C\:\\Users\\vamsi\\Zotero\\storage\\N7PJ9SK2\\ma15a.html:text/html}
}

@article{jain_data_1999,
	title = {Data clustering: a review},
	volume = {31},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Data clustering},
	url = {https://dl.acm.org/doi/10.1145/331499.331504},
	doi = {10.1145/331499.331504},
	language = {en},
	number = {3},
	urldate = {2020-12-17},
	journal = {ACM Computing Surveys},
	author = {Jain, A. K. and Murty, M. N. and Flynn, P. J.},
	month = sep,
	year = {1999},
	pages = {264--323},
	file = {Jain et al. - 1999 - Data clustering a review.pdf:C\:\\Users\\vamsi\\Zotero\\storage\\7M3ZRCHI\\Jain et al. - 1999 - Data clustering a review.pdf:application/pdf}
}

@misc{noauthor_anthropology_nodate,
	title = {Anthropology {Publications}},
	url = {https://dpg.lib.berkeley.edu/webdb/anthpubs/search?all=&volume=31&journal=1&item=5},
	urldate = {2020-12-17},
	file = {Anthropology Publications:C\:\\Users\\vamsi\\Zotero\\storage\\ZIYFYMHY\\search.html:text/html}
}

@article{socqiety_agw-m_nodate,
	title = {{AGW}-{M} {Antbropologische} {Gesellschat} in {Wienp}, ti gen.},
	language = {en},
	author = {Socqiety, EthnolQgical},
	pages = {54},
	file = {Socqiety - AGW-M Antbropologische Gesellschat in Wienp, ti ge.pdf:C\:\\Users\\vamsi\\Zotero\\storage\\HDSW39VX\\Socqiety - AGW-M Antbropologische Gesellschat in Wienp, ti ge.pdf:application/pdf}
}

@article{fraley_model-based_2002,
	title = {Model-{Based} {Clustering}, {Discriminant} {Analysis}, and {Density} {Estimation}},
	volume = {97},
	issn = {0162-1459},
	url = {https://doi.org/10.1198/016214502760047131},
	doi = {10.1198/016214502760047131},
	abstract = {Cluster analysis is the automated search for groups of related observations in a dataset. Most clustering done in practice is based largely on heuristic but intuitively reasonable procedures, and most clustering methods available in commercial software are also of this type. However, there is little systematic guidance associated with these methods for solving important practical questions that arise in cluster analysis, such as how many clusters are there, which clustering method should be used, and how should outliers be handled. We review a general methodology for model-based clustering that provides a principled statistical approach to these issues. We also show that this can be useful for other problems in multivariate analysis, such as discriminant analysis and multivariate density estimation. We give examples from medical diagnosis, minefield detection, cluster recovery from noisy data, and spatial density estimation. Finally, we mention limitations of the methodology and discuss recent developments in model-based clustering for non-Gaussian data, high-dimensional datasets, large datasets, and Bayesian estimation.},
	number = {458},
	urldate = {2020-12-17},
	journal = {Journal of the American Statistical Association},
	author = {Fraley, Chris and Raftery, Adrian E.},
	month = jun,
	year = {2002},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1198/016214502760047131},
	keywords = {Bayes factor, Breast cancer diagnosis, Cluster analysis, EM algorithm, Gene expression microarray data, Markov chain Monte Carlo, Mixture model, Outliers, Spatial point process},
	pages = {611--631},
	file = {Full Text PDF:C\:\\Users\\vamsi\\Zotero\\storage\\3N3XPVII\\Fraley and Raftery - 2002 - Model-Based Clustering, Discriminant Analysis, and.pdf:application/pdf;Snapshot:C\:\\Users\\vamsi\\Zotero\\storage\\RKY4MTYZ\\016214502760047131.html:text/html}
}

@article{dave_robust_1997,
	title = {Robust clustering methods: a unified view},
	volume = {5},
	issn = {1941-0034},
	shorttitle = {Robust clustering methods},
	doi = {10.1109/91.580801},
	abstract = {Clustering methods need to be robust if they are to be useful in practice. In this paper, we analyze several popular robust clustering methods and show that they have much in common. We also establish a connection between fuzzy set theory and robust statistics, and point out the similarities between robust clustering methods and statistical methods such as the weighted least-squares technique, the M estimator, the minimum volume ellipsoid algorithm, cooperative robust estimation, minimization of probability of randomness, and the epsilon contamination model. By gleaning the common principles upon which the methods proposed in the literature are based, we arrive at a unified view of robust clustering methods. We define several general concepts that are useful in robust clustering, state the robust clustering problem in terms of the defined concepts, and propose generic algorithms and guidelines for clustering noisy data. We also discuss why the generalized Hough transform is a suboptimal solution to the robust clustering problem.},
	number = {2},
	journal = {IEEE Transactions on Fuzzy Systems},
	author = {Dave, R. N. and Krishnapuram, R.},
	month = may,
	year = {1997},
	note = {Conference Name: IEEE Transactions on Fuzzy Systems},
	keywords = {Clustering algorithms, Clustering methods, Contamination, Ellipsoids, epsilon contamination model, fuzzy set theory, Fuzzy set theory, Hough transform, Hough transforms, least squares approximations, least-squares technique, minimisation, minimization, Minimization methods, minimum volume ellipsoid, pattern recognition, probability, Probability, robust clustering, robust estimation, Robustness, statistical analysis, Statistical analysis, statistics, Statistics},
	pages = {270--293},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\vamsi\\Zotero\\storage\\2M7CWIY8\\Dave and Krishnapuram - 1997 - Robust clustering methods a unified view.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\vamsi\\Zotero\\storage\\RYUCWMXF\\580801.html:text/html}
}

@article{gan_k_2017,
	title = {k -means clustering with outlier removal},
	volume = {90},
	issn = {01678655},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167865517300740},
	doi = {10.1016/j.patrec.2017.03.008},
	language = {en},
	urldate = {2020-12-17},
	journal = {Pattern Recognition Letters},
	author = {Gan, Guojun and Ng, Michael Kwok-Po},
	month = apr,
	year = {2017},
	pages = {8--14},
	file = {Gan and Ng - 2017 - k -means clustering with outlier removal.pdf:C\:\\Users\\vamsi\\Zotero\\storage\\ESFM5QNB\\Gan and Ng - 2017 - k -means clustering with outlier removal.pdf:application/pdf}
}

@article{jiang_giniclust_2016,
	title = {{GiniClust}: detecting rare cell types from single-cell gene expression data with {Gini} index},
	volume = {17},
	issn = {1474-760X},
	shorttitle = {{GiniClust}},
	url = {https://doi.org/10.1186/s13059-016-1010-4},
	doi = {10.1186/s13059-016-1010-4},
	abstract = {High-throughput single-cell technologies have great potential to discover new cell types; however, it remains challenging to detect rare cell types that are distinct from a large population. We present a novel computational method, called GiniClust, to overcome this challenge. Validation against a benchmark dataset indicates that GiniClust achieves high sensitivity and specificity. Application of GiniClust to public single-cell RNA-seq datasets uncovers previously unrecognized rare cell types, including Zscan4-expressing cells within mouse embryonic stem cells and hemoglobin-expressing cells in the mouse cortex and hippocampus. GiniClust also correctly detects a small number of normal cells that are mixed in a cancer cell population.},
	number = {1},
	urldate = {2020-12-17},
	journal = {Genome Biology},
	author = {Jiang, Lan and Chen, Huidong and Pinello, Luca and Yuan, Guo-Cheng},
	month = jul,
	year = {2016},
	keywords = {Clustering, Gini index, qPCR, Rare cell type, RNA-seq, Single-cell analysis},
	pages = {144},
	file = {Full Text PDF:C\:\\Users\\vamsi\\Zotero\\storage\\ZKR5TGI4\\Jiang et al. - 2016 - GiniClust detecting rare cell types from single-c.pdf:application/pdf}
}

@inproceedings{hautamaki_improving_2005,
	title = {Improving {K}-{Means} by {Outlier} {Removal}},
	volume = {3540},
	isbn = {978-3-540-26320-3},
	doi = {10.1007/11499145_99},
	abstract = {We present an Outlier Removal Clustering (ORC) algorithm that provides outlier detection and data clustering simultaneously.
The method employs both clustering and outlier discovery to improve estimation of the centroids of the generative distribution.
The proposed algorithm consists of two stages. The first stage consist of purely K-means process, while the second stage iteratively
removes the vectors which are far from their cluster centroids. We provide experimental results on three different synthetic
datasets and three map images which were corrupted by lossy compression. The results indicate that the proposed method has
a lower error on datasets with overlapping clusters than the competing methods.},
	author = {Hautamäki, Ville and Drapkina, Svetlana and Kärkkäinen, Ismo and Kinnunen, Tomi},
	month = jun,
	year = {2005},
	pages = {978--987},
	file = {Full Text PDF:C\:\\Users\\vamsi\\Zotero\\storage\\TXSHBMTV\\Hautamäki et al. - 2005 - Improving K-Means by Outlier Removal.pdf:application/pdf}
}

@article{murtagh_survey_1983,
	title = {A {Survey} of {Recent} {Advances} in {Hierarchical} {Clustering} {Algorithms}},
	volume = {26},
	issn = {0010-4620, 1460-2067},
	url = {https://academic.oup.com/comjnl/article-lookup/doi/10.1093/comjnl/26.4.354},
	doi = {10.1093/comjnl/26.4.354},
	language = {en},
	number = {4},
	urldate = {2020-12-17},
	journal = {The Computer Journal},
	author = {Murtagh, F.},
	month = nov,
	year = {1983},
	pages = {354--359},
	file = {Murtagh - 1983 - A Survey of Recent Advances in Hierarchical Cluste.pdf:C\:\\Users\\vamsi\\Zotero\\storage\\7WNZF8DB\\Murtagh - 1983 - A Survey of Recent Advances in Hierarchical Cluste.pdf:application/pdf}
}

@misc{noauthor_step-wise_nodate,
	title = {Step-{Wise} {Clustering} {Procedures}: {Journal} of the {American} {Statistical} {Association}: {Vol} 62, {No} 317},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1967.10482890},
	urldate = {2020-12-17},
	file = {Step-Wise Clustering Procedures\: Journal of the American Statistical Association\: Vol 62, No 317:C\:\\Users\\vamsi\\Zotero\\storage\\675DHUN4\\01621459.1967.html:text/html}
}

@article{king_step-wise_1967,
	title = {Step-{Wise} {Clustering} {Procedures}},
	volume = {62},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2282912},
	doi = {10.2307/2282912},
	abstract = {A simple step-wise procedure for the clustering of variables is described. Two alternative criteria for the merger of groups at each pass are discussed: (1) maximization of the pairwise correlation between the centroids of two groups, and (2) minimization of Wilks' statistic to test the hypothesis of independence between two groups. For a set of sample covariance matrices the step-wise solution for each criterion is compared with the optimal two-group separation of variables found by total enumeration of the possible groupings.},
	number = {317},
	urldate = {2020-12-17},
	journal = {Journal of the American Statistical Association},
	author = {King, Benjamin},
	year = {1967},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {86--101}
}

@article{ward_hierarchical_1963,
	title = {Hierarchical {Grouping} to {Optimize} an {Objective} {Function}},
	volume = {58},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2282967},
	doi = {10.2307/2282967},
	abstract = {A procedure for forming hierarchical groups of mutually exclusive subsets, each of which has members that are maximally similar with respect to specified characteristics, is suggested for use in large-scale ({\textless}latex{\textgreater}\$n {\textgreater} 100\${\textless}/latex{\textgreater}) studies when a precise optimal solution for a specified number of groups is not practical. Given n sets, this procedure permits their reduction to n - 1 mutually exclusive sets by considering the union of all possible n(n - 1)/2 pairs and selecting a union having a maximal value for the functional relation, or objective function, that reflects the criterion chosen by the investigator. By repeating this process until only one group remains, the complete hierarchical structure and a quantitative estimate of the loss associated with each stage in the grouping can be obtained. A general flowchart helpful in computer programming and a numerical example are included.},
	number = {301},
	urldate = {2020-12-17},
	journal = {Journal of the American Statistical Association},
	author = {Ward, Joe H.},
	year = {1963},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {236--244}
}

@article{tibshirani_valerie_nodate,
	title = {Valerie and {Patrick} {Hastie}},
	language = {en},
	author = {Tibshirani, Sami and Friedman, Harry},
	pages = {764},
	file = {Tibshirani and Friedman - Valerie and Patrick Hastie.pdf:C\:\\Users\\vamsi\\Zotero\\storage\\CK9LQJ77\\Tibshirani and Friedman - Valerie and Patrick Hastie.pdf:application/pdf}
}

@article{mullner_fastcluster_2013,
	title = {\textbf{fastcluster} : {Fast} {Hierarchical}, {Agglomerative} {Clustering} {Routines} for \textit{{R}} and \textit{{Python}}},
	volume = {53},
	issn = {1548-7660},
	shorttitle = {\textbf{fastcluster}},
	url = {http://www.jstatsoft.org/v53/i09/},
	doi = {10.18637/jss.v053.i09},
	language = {en},
	number = {9},
	urldate = {2020-12-17},
	journal = {Journal of Statistical Software},
	author = {Müllner, Daniel},
	year = {2013},
	file = {Müllner - 2013 - fastcluster  Fast Hierarchical, Agglomerat.pdf:C\:\\Users\\vamsi\\Zotero\\storage\\ARZYNCGD\\Müllner - 2013 - fastcluster  Fast Hierarchical, Agglomerat.pdf:application/pdf}
}

@article{jolliffe_discarding_1972,
	title = {Discarding {Variables} in a {Principal} {Component} {Analysis}. {I}: {Artificial} {Data}},
	volume = {21},
	issn = {0035-9254},
	shorttitle = {Discarding {Variables} in a {Principal} {Component} {Analysis}. {I}},
	url = {https://www.jstor.org/stable/2346488},
	doi = {10.2307/2346488},
	abstract = {Often, results obtained from the use of principal component analysis are little changed if some of the variables involved are discarded beforehand. This paper examines some of the possible methods for deciding which variables to reject and these rejection methods are tested on artificial data containing variables known to be "redundant". It is shown that several of the rejection methods, of differing types, each discard precisely those variables known to be redundant, for all but a few sets of data.},
	number = {2},
	urldate = {2020-12-17},
	journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
	author = {Jolliffe, I. T.},
	year = {1972},
	note = {Publisher: [Wiley, Royal Statistical Society]},
	pages = {160--173}
}

@article{penrose_generalized_1955,
	title = {A generalized inverse for matrices},
	volume = {51},
	issn = {1469-8064, 0305-0041},
	url = {https://www.cambridge.org/core/journals/mathematical-proceedings-of-the-cambridge-philosophical-society/article/generalized-inverse-for-matrices/5F4516D6B9989BB6563A4B267CC7D615},
	doi = {10.1017/S0305004100030401},
	abstract = {This paper describes a generalization of the inverse of a non-singular matrix, as the unique solution of a certain set of equations. This generalized inverse exists for any (possibly rectangular) matrix whatsoever with complex elements. It is used here for solving linear matrix equations, and among other applications for finding an expression for the principal idempotent elements of a matrix. Also a new type of spectral decomposition is given.},
	language = {en},
	number = {3},
	urldate = {2020-12-17},
	journal = {Mathematical Proceedings of the Cambridge Philosophical Society},
	author = {Penrose, R.},
	month = jul,
	year = {1955},
	note = {Publisher: Cambridge University Press},
	pages = {406--413},
	file = {Full Text PDF:C\:\\Users\\vamsi\\Zotero\\storage\\DZTRYRZP\\Penrose - 1955 - A generalized inverse for matrices.pdf:application/pdf;Snapshot:C\:\\Users\\vamsi\\Zotero\\storage\\32KCVKMD\\5F4516D6B9989BB6563A4B267CC7D615.html:text/html}
}

@article{breiman_heuristics_1996,
	title = {Heuristics of instability and stabilization in model selection},
	volume = {24},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1032181158},
	doi = {10.1214/aos/1032181158},
	abstract = {In model selection, usually a "best" predictor is chosen from a collection μ{\textasciicircum}(⋅,s)μ{\textasciicircum}(⋅,s)\{{\textbackslash}hat\{{\textbackslash}mu\}({\textbackslash}cdot, s)\} of predictors where μ{\textasciicircum}(⋅,s)μ{\textasciicircum}(⋅,s){\textbackslash}hat\{{\textbackslash}mu\}({\textbackslash}cdot, s) is the minimum least-squares predictor in a collection UsUs{\textbackslash}mathsf\{U\}\_s of predictors. Here s is a complexity parameter; that is, the smaller s, the lower dimensional/smoother the models in UsUs{\textbackslash}mathsf\{U\}\_s. If LL{\textbackslash}mathsf\{L\} is the data used to derive the sequence μ{\textasciicircum}(⋅,s)μ{\textasciicircum}(⋅,s)\{{\textbackslash}hat\{{\textbackslash}mu\}({\textbackslash}cdot, s)\}, the procedure is called unstable if a small change in LL{\textbackslash}mathsf\{L\} can cause large changes in μ{\textasciicircum}(⋅,s)μ{\textasciicircum}(⋅,s)\{{\textbackslash}hat\{{\textbackslash}mu\}({\textbackslash}cdot, s)\}. With a crystal ball, one could pick the predictor in μ{\textasciicircum}(⋅,s)μ{\textasciicircum}(⋅,s)\{{\textbackslash}hat\{{\textbackslash}mu\}({\textbackslash}cdot, s)\} having minimum prediction error. Without prescience, one uses test sets, cross-validation and so forth. The difference in prediction error between the crystal ball selection and the statistician's choice we call predictive loss. For an unstable procedure the predictive loss is large. This is shown by some analytics in a simple case and by simulation results in a more complex comparison of four different linear regression methods. Unstable procedures can be stabilized by perturbing the data, getting a new predictor sequence μ′{\textasciicircum}(⋅,s)μ′{\textasciicircum}(⋅,s)\{{\textbackslash}hat\{{\textbackslash}mu'\}({\textbackslash}cdot, s)\} and then averaging over many such predictor sequences.},
	language = {en},
	number = {6},
	urldate = {2020-12-17},
	journal = {Annals of Statistics},
	author = {Breiman, Leo},
	month = dec,
	year = {1996},
	mrnumber = {MR1425957},
	zmnumber = {0867.62055},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {cross-validation, prediction error, predictive loss, Regression, subset selection},
	pages = {2350--2383},
	file = {Snapshot:C\:\\Users\\vamsi\\Zotero\\storage\\R5S6B2HM\\1032181158.html:text/html;Full Text PDF:C\:\\Users\\vamsi\\Zotero\\storage\\HS9DYIWW\\Breiman - 1996 - Heuristics of instability and stabilization in mod.pdf:application/pdf;Snapshot:C\:\\Users\\vamsi\\Zotero\\storage\\CCKXYJEN\\1032181158.html:text/html}
}

@article{hoerl_ridge_1975,
	title = {Ridge regression:some simulations},
	volume = {4},
	issn = {0090-3272},
	shorttitle = {Ridge regression},
	url = {https://doi.org/10.1080/03610927508827232},
	doi = {10.1080/03610927508827232},
	abstract = {An algorithm is given for selacting the biasing paramatar, k, in RIDGE regrassion. By means of simulaction it is shown that the algorithm has the following properties: (i) it produces an aberaged squared error for the regrassion coafficiants that is les than least squares, (ii) the distribuction of squared arrots for the regression coafficiants has a smallar variance than does that for last squares, and (iii) regradless of he signal-to-noiss retio the probability that RIDGE producas a smaller squared error than least squares is greatar than 0.50.},
	number = {2},
	urldate = {2020-12-17},
	journal = {Communications in Statistics},
	author = {Hoerl, Arthur E. and Kannard, Robert W. and Baldwin, Kent F.},
	month = jan,
	year = {1975},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/03610927508827232},
	keywords = {biassed regression estimators, multiple linear regression, ridge regression, ridge trace},
	pages = {105--123},
	file = {Snapshot:C\:\\Users\\vamsi\\Zotero\\storage\\499XYC6G\\03610927508827232.html:text/html;Snapshot:C\:\\Users\\vamsi\\Zotero\\storage\\HC6V5CUA\\03610927508827232.html:text/html}
}


@article{roorkiwal_genome-enabled_2016,
	title = {Genome-{Enabled} {Prediction} {Models} for {Yield} {Related} {Traits} in {Chickpea}},
	volume = {7},
	issn = {1664-462X},
	url = {https://www.frontiersin.org/articles/10.3389/fpls.2016.01666/full},
	doi = {10.3389/fpls.2016.01666},
	abstract = {Genomic selection (GS) unlike marker-assisted backcrossing (MABC) predicts breeding values of lines using genome-wide marker profiling and allows selection of lines prior to field-phenotyping, thereby shortening the breeding cycle. A collection of 320 elite breeding lines was selected and phenotyped extensively for yield and yield related traits at two different locations (Delhi and Patancheru, India) during the crop seasons 2011-12 and 2012-13 under rainfed and irrigated conditions. In parallel, these lines were also genotyped using DArTseq platform to generate data on 3,000 polymorphic markers. Phenotypic and genotypic data were used with six statistical GS models to estimate the prediction accuracies. GS models were tested for four yield related traits viz. seed yield, 100 seed weight, days to 50\% flowering and days to maturity. Prediction accuracy for the models tested varied from 0.138 (seed yield) to 0.912 (100 seed weight), whereas performance of models did not show any significant difference for estimating prediction accuracy within traits. Kinship matrix calculated using genotyping data reaffirmed existence of two different groups within selected lines. There was not much effect of population structure on prediction accuracy. In brief, present study establishes the necessary resources for deployment of GS in chickpea breeding.},
	language = {English},
	urldate = {2020-12-22},
	journal = {Frontiers in Plant Science},
	author = {Roorkiwal, Manish and Rathore, Abhishek and Das, Roma R. and Singh, Muneendra K. and Jain, Ankit and Srinivasan, Samineni and Gaur, Pooran M. and Chellapilla, Bharadwaj and Tripathi, Shailesh and Li, Yongle and Hickey, John M. and Lorenz, Aaron and Sutton, Tim and Crossa, Jose and Jannink, Jean-Luc and Varshney, Rajeev K.},
	year = {2016},
	note = {Publisher: Frontiers},
	keywords = {chickpea, Genetic gain, Genomic prediction accuracy, genomic selection, population structure, Prediction models, training population},
	file = {Full Text PDF:C\:\\Users\\vamsi\\Zotero\\storage\\MEM4CW4M\\Roorkiwal et al. - 2016 - Genome-Enabled Prediction Models for Yield Related.pdf:application/pdf}
}

@inproceedings{osg07,
  title  = {The open science grid},
  author = {
    Pordes, Ruth 
    and Petravick, Don 
    and Kramer, Bill 
    and Olson, Doug 
    and Livny, Miron 
    and Roy, Alain 
    and Avery, Paul 
    and Blackburn, Kent 
    and Wenaus, Torre 
    and W{\"u}rthwein, Frank 
    and Foster, Ian
    and Gardner, Rob
    and Wilde, Mike
    and Blatecky, Alan
    and McGee, John
    and Quick, Rob
  },
  doi       = {10.1088/1742-6596/78/1/012057},
  booktitle = {J. Phys. Conf. Ser.},
  volume    = {78},
  series    = {78},
  pages     = {012057},
  year      = {2007},
}

@inproceedings{osg09,
  title        = {The pilot way to grid resources using glideinWMS},
  author       = {
    Sfiligoi, Igor    
    and Bradley, Daniel C 
    and Holzman, Burt     
    and Mhashilkar, Parag 
    and Padhi, Sanjay     
    and Wurthwein, Frank
  },
  doi          = {10.1109/CSIE.2009.950},
  booktitle    = {2009 WRI World Congress on Computer Science and Information Engineering},
  volume       = {2},
  series       = {2},
  pages        = {428--432},
  year         = {2009},
}

@book{chatterjee1988,
author = {Chatterjee, Samprit},
title = {Sensitivity Analysis in Linear Regression},
year = {1988},
isbn = {0471822167},
publisher = {John Wiley \& Sons, Inc.},
address = {USA}
}

@Manual{Rcite,
title = {R: A Language and Environment for Statistical Computing},
author = {{R Core Team}},
organization = {R Foundation for Statistical Computing},
address = {Vienna, Austria},
year = {2020},
url = {https://www.R-project.org/},
}

@Manual{RaProR,
    title = {RaProR: Calculate Sketches using Random Projections to Reduce Large Data
Sets},
    author = {Leo N. Geppert and Katja Ickstadt and Alexander Munteanu and Jens Quedenfeld and Ludger Sandig and Christian Sohler},
    year = {2019},
    note = {R package version 1.1-5},
    url = {https://CRAN.R-project.org/package=RaProR},
  }

@article{penrose_1955, 
title={A generalized inverse for matrices}, volume={51}, DOI={10.1017/S0305004100030401}, number={3}, 
journal={Mathematical Proceedings of the Cambridge Philosophical Society}, 
publisher={Cambridge University Press}, 
author={Penrose, R.}, 
year={1955},
pages={406–413}
}

@Article{glmnet,
    title = {Regularization Paths for Cox's Proportional Hazards Model via Coordinate Descent},
    author = {Noah Simon and Jerome Friedman and Trevor Hastie and Rob Tibshirani},
    journal = {Journal of Statistical Software},
    year = {2011},
    volume = {39},
    number = {5},
    pages = {1--13},
    url = {http://www.jstatsoft.org/v39/i05/},
  }
  
@article{pearson1901,
author = { Karl   Pearson},
title = {LIII. On lines and planes of closest fit to systems of points in space},
journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
volume = {2},
number = {11},
pages = {559-572},
year  = {1901},
publisher = {Taylor \& Francis},
doi = {10.1080/14786440109462720},
}

@article{JL1984,
author = {Johnson, William and Lindenstrauss, Joram},
year = {1984},
month = {01},
pages = {189-206},
title = {Extensions of Lipschitz maps into a Hilbert space},
volume = {26},
isbn = {9780821850305},
journal = {Contemporary Mathematics},
doi = {10.1090/conm/026/737400}
}

@article{frankl1988,
title = {The Johnson-Lindenstrauss lemma and the sphericity of some graphs},
journal = {Journal of Combinatorial Theory, Series B},
volume = {44},
number = {3},
pages = {355-362},
year = {1988},
issn = {0095-8956},
doi = {https://doi.org/10.1016/0095-8956(88)90043-3},
url = {https://www.sciencedirect.com/science/article/pii/0095895688900433},
author = {P Frankl and H Maehara},
abstract = {A simple short proof of the Johnson-Lindenstrauss lemma (concerning nearly isometric embeddings of finite point sets in lower-dimensional spaces) is given. This result is applied to show that if G is a graph on n vertices and with smallest eigenvalue λ then its sphericity sph(G) is less than cλ2 log n. It is also proved that if G or its complement is a forest then sph(G) ≤ c log n holds.}
}

@book{Hartigan1975,
author = {Hartigan, John A.},
title = {Clustering Algorithms},
year = {1975},
isbn = {047135645X},
publisher = {John Wiley \& Sons, Inc.},
address = {USA},
edition = {99th}
}

@book{eslr,
  added-at = {2008-05-16T16:17:42.000+0200},
  address = {New York, NY, USA},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  biburl = {https://www.bibsonomy.org/bibtex/2f58afc5c9793fcc8ad8389824e57984c/sb3000},
  interhash = {d585aea274f2b9b228fc1629bc273644},
  intrahash = {f58afc5c9793fcc8ad8389824e57984c},
  keywords = {ml statistics},
  publisher = {Springer New York Inc.},
  series = {Springer Series in Statistics},
  timestamp = {2008-05-16T16:17:43.000+0200},
  title = {The Elements of Statistical Learning},
  year = 2001
}

@book{meyer2000,
author = {Meyer, Carl D.},
title = {Matrix Analysis and Applied Linear Algebra},
year = {2000},
isbn = {0898714540},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA}
}

@article{tibshirani1996,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2346178},
 abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
 author = {Robert Tibshirani},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {1},
 pages = {267--288},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Regression Shrinkage and Selection via the Lasso},
 volume = {58},
 year = {1996}
}

@article{Punj1983,
author = {Girish Punj and David W. Stewart},
title ={Cluster Analysis in Marketing Research: Review and Suggestions for Application},
journal = {Journal of Marketing Research},
volume = {20},
number = {2},
pages = {134-148},
year = {1983},
doi = {10.1177/002224378302000204},

URL = { 
        https://doi.org/10.1177/002224378302000204
    
},
eprint = { 
        https://doi.org/10.1177/002224378302000204
    
}
,
    abstract = { Applications of cluster analysis to marketing problems are reviewed. Alternative methods of cluster analysis are presented and evaluated in terms of recent empirical work on their performance characteristics. A two-stage cluster analysis methodology is recommended: preliminary identification of clusters via Ward's minimum variance method or simple average linkage, followed by cluster refinement by an iterative partitioning procedure. Issues and problems related to the use and validation of cluster analytic methods are discussed. }
}

@article{Sutherland2012,
    doi = {10.1371/journal.pone.0036631},
    author = {Sutherland, E. Rand AND Goleva, Elena AND King, Tonya S. AND Lehman, Erik AND Stevens, Allen D. AND Jackson, Leisa P. AND Stream, Amanda R. AND Fahy, John V. AND Donald Y. M. Leung2,4 for the Asthma Clinical Research Network},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Cluster Analysis of Obesity and Asthma Phenotypes},
    year = {2012},
    month = {05},
    volume = {7},
    url = {https://doi.org/10.1371/journal.pone.0036631},
    pages = {1-7},
    abstract = {Background Asthma is a heterogeneous disease with variability among patients in characteristics such as lung function, symptoms and control, body weight, markers of inflammation, and responsiveness to glucocorticoids (GC). Cluster analysis of well-characterized cohorts can advance understanding of disease subgroups in asthma and point to unsuspected disease mechanisms. We utilized an hypothesis-free cluster analytical approach to define the contribution of obesity and related variables to asthma phenotype.   Methodology and Principal Findings In a cohort of clinical trial participants (n = 250), minimum-variance hierarchical clustering was used to identify clinical and inflammatory biomarkers important in determining disease cluster membership in mild and moderate persistent asthmatics. In a subset of participants, GC sensitivity was assessed via expression of GC receptor alpha (GCRα) and induction of MAP kinase phosphatase-1 (MKP-1) expression by dexamethasone. Four asthma clusters were identified, with body mass index (BMI, kg/m2) and severity of asthma symptoms (AEQ score) the most significant determinants of cluster membership (F = 57.1, p<0.0001 and F = 44.8, p<0.0001, respectively). Two clusters were composed of predominantly obese individuals; these two obese asthma clusters differed from one another with regard to age of asthma onset, measures of asthma symptoms (AEQ) and control (ACQ), exhaled nitric oxide concentration (FENO) and airway hyperresponsiveness (methacholine PC20) but were similar with regard to measures of lung function (FEV1 (%) and FEV1/FVC), airway eosinophilia, IgE, leptin, adiponectin and C-reactive protein (hsCRP). Members of obese clusters demonstrated evidence of reduced expression of GCRα, a finding which was correlated with a reduced induction of MKP-1 expression by dexamethasone   Conclusions and Significance Obesity is an important determinant of asthma phenotype in adults. There is heterogeneity in expression of clinical and inflammatory biomarkers of asthma across obese individuals. Reduced expression of the dominant functional isoform of the GCR may mediate GC insensitivity in obese asthmatics.},
    number = {5},

}

@article{Clatworthy2005,
author = {Clatworthy, Jane. and Buick, Deanna. and Hankins, Matthew. and Weinman, John. and Horne, Robert.},
title = {The use and reporting of cluster analysis in health psychology: A review},
journal = {British Journal of Health Psychology},
volume = {10},
number = {3},
pages = {329-358},
doi = {https://doi.org/10.1348/135910705X25697},
url = {https://bpspsychub.onlinelibrary.wiley.com/doi/abs/10.1348/135910705X25697},
eprint = {https://bpspsychub.onlinelibrary.wiley.com/doi/pdf/10.1348/135910705X25697},
abstract = {Purpose Cluster analysis is a collection of relatively simple descriptive statistical techniques with potential value in health psychology, addressing both theoretical and practical problems. There are many methods of cluster analysis from which to choose, with no clear guidelines to aid researchers. In the absence of guidelines it is likely that methods already reported by published researchers will be adopted, and so clear reporting of statistical methodology, while always important, is particularly crucial with cluster analysis. The aim of this review is to describe and evaluate the reporting of cluster analysis in health psychology publications. Methods Electronic searches of 18 health psychology journals identified 59 articles using cluster analysis published between 1984 and 2002. Articles were submitted to systematic evaluation against published criteria for the reporting of cluster analysis. Results Just 27\% of the papers reviewed met all five criteria, although 61\% met at least four. Details of the similarity measure and the computer program used were most frequently omitted. Furthermore, while researchers usually reported the procedures employed to determine the number of clusters and to validate the clusters, these procedures were often lacking in rigour, and were reported in insufficient detail for replication. Conclusions The reporting of cluster analysis was found to be generally unsatisfactory, with many studies failing to provide enough information to allow replication or the evaluation of the quality of the research. Clear guidelines for conducting and reporting cluster analyses in health psychology are needed.},
year = {2005}
}

@ARTICLE{Jiang2004,
  author={Daxin Jiang and Chun Tang and Aidong Zhang},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Cluster analysis for gene expression data: a survey}, 
  year={2004},
  volume={16},
  number={11},
  pages={1370-1386},
  doi={10.1109/TKDE.2004.68}}
  
@book{Jain1988,
author = {Jain, Anil K. and Dubes, Richard C.},
title = {Algorithms for Clustering Data},
year = {1988},
isbn = {013022278X},
publisher = {Prentice-Hall, Inc.},
address = {USA}
}

@article{hayes_accuracy_2009,
	title = {Accuracy of genomic breeding values in multi-breed dairy cattle populations},
	volume = {41},
	issn = {1297-9686},
	url = {https://doi.org/10.1186/1297-9686-41-51},
	doi = {10.1186/1297-9686-41-51},
	abstract = {Two key findings from genomic selection experiments are 1) the reference population used must be very large to subsequently predict accurate genomic estimated breeding values (GEBV), and 2) prediction equations derived in one breed do not predict accurate GEBV when applied to other breeds. Both findings are a problem for breeds where the number of individuals in the reference population is limited. A multi-breed reference population is a potential solution, and here we investigate the accuracies of GEBV in Holstein dairy cattle and Jersey dairy cattle when the reference population is single breed or multi-breed. The accuracies were obtained both as a function of elements of the inverse coefficient matrix and from the realised accuracies of GEBV.},
	number = {1},
	urldate = {2021-09-12},
	journal = {Genetics Selection Evolution},
	author = {Hayes, Ben J. and Bowman, Phillip J. and Chamberlain, Amanda C. and Verbyla, Klara and Goddard, Mike E.},
	month = nov,
	year = {2009},
	keywords = {Best Linear Unbiased Prediction, Genomic Estimate Breeding Value, Genomic Selection, Quantitative Trait Locus, Reference Population},
	pages = {51},
	file = {Full Text PDF:C\:\\Users\\vamsi\\Zotero\\storage\\776QYFSR\\Hayes et al. - 2009 - Accuracy of genomic breeding values in multi-breed.pdf:application/pdf;Snapshot:C\:\\Users\\vamsi\\Zotero\\storage\\QLTKDHTC\\1297-9686-41-51.html:text/html},
}

@article{park_casella_2008,
author = {Trevor Park and George Casella},
title = {The Bayesian Lasso},
journal = {Journal of the American Statistical Association},
volume = {103},
number = {482},
pages = {681-686},
year  = {2008},
publisher = {Taylor & Francis},
doi = {10.1198/016214508000000337},

URL = { 
        https://doi.org/10.1198/016214508000000337
    
},
eprint = { 
        https://doi.org/10.1198/016214508000000337
    
}

}

@article{gianola_predicting_2011,
	title = {Predicting complex quantitative traits with {Bayesian} neural networks: a case study with {Jersey} cows and wheat},
	volume = {12},
	issn = {1471-2156},
	shorttitle = {Predicting complex quantitative traits with {Bayesian} neural networks},
	url = {https://doi.org/10.1186/1471-2156-12-87},
	doi = {10.1186/1471-2156-12-87},
	abstract = {In the study of associations between genomic data and complex phenotypes there may be relationships that are not amenable to parametric statistical modeling. Such associations have been investigated mainly using single-marker and Bayesian linear regression models that differ in their distributions, but that assume additive inheritance while ignoring interactions and non-linearity. When interactions have been included in the model, their effects have entered linearly. There is a growing interest in non-parametric methods for predicting quantitative traits based on reproducing kernel Hilbert spaces regressions on markers and radial basis functions. Artificial neural networks (ANN) provide an alternative, because these act as universal approximators of complex functions and can capture non-linear relationships between predictors and responses, with the interplay among variables learned adaptively. ANNs are interesting candidates for analysis of traits affected by cryptic forms of gene action.},
	number = {1},
	urldate = {2021-09-12},
	journal = {BMC Genetics},
	author = {Gianola, Daniel and Okut, Hayrettin and Weigel, Kent A. and Rosa, Guilherme JM},
	month = oct,
	year = {2011},
	keywords = {Artificial Neural Network, Connection Strength, Genomic Relationship, Predictive Correlation, Reproduce Kernel Hilbert Space},
	pages = {87},
	file = {Full Text PDF:C\:\\Users\\vamsi\\Zotero\\storage\\SJ3S7BKB\\Gianola et al. - 2011 - Predicting complex quantitative traits with Bayesi.pdf:application/pdf;Snapshot:C\:\\Users\\vamsi\\Zotero\\storage\\69CAS9QD\\1471-2156-12-87.html:text/html},
}

@article{gonzalez_camacho_2012,
author = {Gonzalez-Camacho, Juan and Campos, G and Pérez, P and Gianola, Daniel and Cairns, Jill and Mahuku, George and Babu, Raman and Crossa, Jose},
year = {2012},
month = {05},
pages = {759-71},
title = {Genome-enabled prediction of genetic values using radial basis function neural networks},
volume = {125},
journal = {TAG. Theoretical and applied genetics. Theoretische und angewandte Genetik},
doi = {10.1007/s00122-012-1868-9}
}

@article{perez_rodriguez_2012,
author = {Pérez-Rodríguez, Paulino and Gianola, Daniel and Gonzalez-Camacho, Juan and Crossa, Jose and Manes, Yann and Dreisigacker, Susanne},
year = {2012},
month = {12},
pages = {1595-605},
title = {Comparison Between Linear and Non-parametric Regression Models for Genome-Enabled Prediction in Wheat},
volume = {2},
journal = {G3 (Bethesda, Md.)},
doi = {10.1534/g3.112.003665}
}

@article {de_los_campos_2013,
	author = {de los Campos, Gustavo and Hickey, John M. and Pong-Wong, Ricardo and Daetwyler, Hans D. and Calus, Mario P. L.},
	title = {Whole-Genome Regression and Prediction Methods Applied to Plant and Animal Breeding},
	volume = {193},
	number = {2},
	pages = {327--345},
	year = {2013},
	doi = {10.1534/genetics.112.143313},
	publisher = {Genetics},
	abstract = {Genomic-enabled prediction is becoming increasingly important in animal and plant breeding and is also receiving attention in human genetics. Deriving accurate predictions of complex traits requires implementing whole-genome regression (WGR) models where phenotypes are regressed on thousands of markers concurrently. Methods exist that allow implementing these large-p with small-n regressions, and genome-enabled selection (GS) is being implemented in several plant and animal breeding programs. The list of available methods is long, and the relationships between them have not been fully addressed. In this article we provide an overview of available methods for implementing parametric WGR models, discuss selected topics that emerge in applications, and present a general discussion of lessons learned from simulation and empirical data analysis in the last decade.},
	issn = {0016-6731},
	URL = {https://www.genetics.org/content/193/2/327},
	eprint = {https://www.genetics.org/content/193/2/327.full.pdf},
	journal = {Genetics}
}


@article{azodi_benchmarking_2019,
	title = {Benchmarking {Parametric} and {Machine} {Learning} {Models} for {Genomic} {Prediction} of {Complex} {Traits}},
	volume = {9},
	issn = {2160-1836},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6829122/},
	doi = {10.1534/g3.119.400498},
	abstract = {The usefulness of genomic prediction in crop and livestock breeding programs has prompted efforts to develop new and improved genomic prediction algorithms, such as artificial neural networks and gradient tree boosting. However, the performance of these algorithms has not been compared in a systematic manner using a wide range of datasets and models. Using data of 18 traits across six plant species with different marker densities and training population sizes, we compared the performance of six linear and six non-linear algorithms. First, we found that hyperparameter selection was necessary for all non-linear algorithms and that feature selection prior to model training was critical for artificial neural networks when the markers greatly outnumbered the number of training lines. Across all species and trait combinations, no one algorithm performed best, however predictions based on a combination of results from multiple algorithms (i.e., ensemble predictions) performed consistently well. While linear and non-linear algorithms performed best for a similar number of traits, the performance of non-linear algorithms vary more between traits. Although artificial neural networks did not perform best for any trait, we identified strategies (i.e., feature selection, seeded starting weights) that boosted their performance to near the level of other algorithms. Our results highlight the importance of algorithm selection for the prediction of trait values.},
	number = {11},
	urldate = {2021-09-12},
	journal = {G3: Genes{\textbar}Genomes{\textbar}Genetics},
	author = {Azodi, Christina B. and Bolger, Emily and McCarren, Andrew and Roantree, Mark and de los Campos, Gustavo and Shiu, Shin-Han},
	month = sep,
	year = {2019},
	pmid = {31533955},
	pmcid = {PMC6829122},
	pages = {3691--3702},
	file = {PubMed Central Full Text PDF:C\:\\Users\\vamsi\\Zotero\\storage\\7C9MJNUS\\Azodi et al. - 2019 - Benchmarking Parametric and Machine Learning Model.pdf:application/pdf},
}

@article{heslot_2012,
author = {Heslot, Nicolas and Yang, Hsiao-Pei and Sorrells, Mark E. and Jannink, Jean-Luc},
title = {Genomic Selection in Plant Breeding: A Comparison of Models},
journal = {Crop Science},
volume = {52},
number = {1},
pages = {146-160},
doi = {https://doi.org/10.2135/cropsci2011.06.0297},
url = {https://acsess.onlinelibrary.wiley.com/doi/abs/10.2135/cropsci2011.06.0297},
eprint = {https://acsess.onlinelibrary.wiley.com/doi/pdf/10.2135/cropsci2011.06.0297},
abstract = {ABSTRACT Simulation and empirical studies of genomic selection (GS) show accuracies sufficient to generate rapid genetic gains. However, with the increased popularity of GS approaches, numerous models have been proposed and no comparative analysis is available to identify the most promising ones. Using eight wheat (Triticum aestivum L.), barley (Hordeum vulgare L.), Arabidopsis thaliana (L.) Heynh., and maize (Zea mays L.) datasets, the predictive ability of currently available GS models along with several machine learning methods was evaluated by comparing accuracies, the genomic estimated breeding values (GEBVs), and the marker effects for each model. While a similar level of accuracy was observed for many models, the level of overfitting varied widely as did the computation time and the distribution of marker effect estimates. Our comparisons suggested that GS in plant breeding programs could be based on a reduced set of models such as the Bayesian Lasso, weighted Bayesian shrinkage regression (wBSR, a fast version of BayesB), and random forest (RF) (a machine learning method that could capture nonadditive effects). Linear combinations of different models were tested as well as bagging and boosting methods, but they did not improve accuracy. This study also showed large differences in accuracy between subpopulations within a dataset that could not always be explained by differences in phenotypic variance and size. The broad diversity of empirical datasets tested here adds evidence that GS could increase genetic gain per unit of time and cost.},
year = {2012}
}

@article{reka_howard_2014,
author = {Howard, Reka and Carriquiry, Alicia and Beavis, William},
year = {2014},
month = {04},
pages = {},
title = {Parametric and Nonparametric Statistical Methods for Genomic Selection of Traits with Additive and Epistatic Genetic Architectures},
volume = {4},
journal = {G3 (Bethesda, Md.)},
doi = {10.1534/g3.114.010298}
}

@article{crain_2018,
author = {Crain, Jared and Mondal, Suchismita and Rutkoski, Jessica and Singh, Ravi P. and Poland, Jesse},
title = {Combining High-Throughput Phenotyping and Genomic Information to Increase Prediction and Selection Accuracy in Wheat Breeding},
journal = {The Plant Genome},
volume = {11},
number = {1},
pages = {170043},
doi = {https://doi.org/10.3835/plantgenome2017.05.0043},
url = {https://acsess.onlinelibrary.wiley.com/doi/abs/10.3835/plantgenome2017.05.0043},
eprint = {https://acsess.onlinelibrary.wiley.com/doi/pdf/10.3835/plantgenome2017.05.0043},
abstract = {Core Ideas Wheat breeding High throughput phenotyping Genomic selection Yield prediction modeling Genomics and phenomics have promised to revolutionize the field of plant breeding. The integration of these two fields has just begun and is being driven through big data by advances in next-generation sequencing and developments of field-based high-throughput phenotyping (HTP) platforms. Each year the International Maize and Wheat Improvement Center (CIMMYT) evaluates tens-of-thousands of advanced lines for grain yield across multiple environments. To evaluate how CIMMYT may utilize dynamic HTP data for genomic selection (GS), we evaluated 1170 of these advanced lines in two environments, drought (2014, 2015) and heat (2015). A portable phenotyping system called ‘Phenocart’ was used to measure normalized difference vegetation index and canopy temperature simultaneously while tagging each data point with precise GPS coordinates. For genomic profiling, genotyping-by-sequencing (GBS) was used for marker discovery and genotyping. Several GS models were evaluated utilizing the 2254 GBS markers along with over 1.1 million phenotypic observations. The physiological measurements collected by HTP, whether used as a response in multivariate models or as a covariate in univariate models, resulted in a range of 33\% below to 7\% above the standard univariate model. Continued advances in yield prediction models as well as increasing data generating capabilities for both genomic and phenomic data will make these selection strategies tractable for plant breeders to implement increasing the rate of genetic gain.},
year = {2018}
}

@article{haroldo_neves_2012,
author = {Neves, Haroldo and Carvalheiro, Roberto and Queiroz, Sandra},
year = {2012},
month = {11},
pages = {100},
title = {A comparison of statistical methods for genomic selection in a mice population},
volume = {13},
journal = {BMC genetics},
doi = {10.1186/1471-2156-13-100}
}

@ARTICLE{li_zhang_2018,
  
AUTHOR={Li, Bo and Zhang, Nanxi and Wang, You-Gan and George, Andrew W. and Reverter, Antonio and Li, Yutao},   
	 
TITLE={Genomic Prediction of Breeding Values Using a Subset of SNPs Identified by Three Machine Learning Methods},      
	
JOURNAL={Frontiers in Genetics},      
	
VOLUME={9},      

PAGES={237},     
	
YEAR={2018},      
	  
URL={https://www.frontiersin.org/article/10.3389/fgene.2018.00237},       
	
DOI={10.3389/fgene.2018.00237},      
	
ISSN={1664-8021},   
   
ABSTRACT={The analysis of large genomic data is hampered by issues such as a small number of observations and a large number of predictive variables (commonly known as “large P small N”), high dimensionality or highly correlated data structures. Machine learning methods are renowned for dealing with these problems. To date machine learning methods have been applied in Genome-Wide Association Studies for identification of candidate genes, epistasis detection, gene network pathway analyses and genomic prediction of phenotypic values. However, the utility of two machine learning methods, Gradient Boosting Machine (GBM) and Extreme Gradient Boosting Method (XgBoost), in identifying a subset of SNP makers for genomic prediction of breeding values has never been explored before. In this study, using 38,082 SNP markers and body weight phenotypes from 2,093 Brahman cattle (1,097 bulls as a discovery population and 996 cows as a validation population), we examined the efficiency of three machine learning methods, namely Random Forests (RF), GBM and XgBoost, in (a) the identification of top 400, 1,000, and 3,000 ranked SNPs; (b) using the subsets of SNPs to construct genomic relationship matrices (GRMs) for the estimation of genomic breeding values (GEBVs). For comparison purposes, we also calculated the GEBVs from (1) 400, 1,000, and 3,000 SNPs that were randomly selected and evenly spaced across the genome, and (2) from all the SNPs. We found that RF and especially GBM are efficient methods in identifying a subset of SNPs with direct links to candidate genes affecting the growth trait. In comparison to the estimate of prediction accuracy of GEBVs from using all SNPs (0.43), the 3,000 top SNPs identified by RF (0.42) and GBM (0.46) had similar values to those of the whole SNP panel. The performance of the subsets of SNPs from RF and GBM was substantially better than that of evenly spaced subsets across the genome (0.18–0.29). Of the three methods, RF and GBM consistently outperformed the XgBoost in genomic prediction accuracy.}
}

@article{montesinos_lopez_2021,
author = {Montesinos-López, Osval and Montesinos, Abelardo and Pérez-Rodríguez, Paulino and Barron Lopez, Jose and Martini, Johannes and Fajardo-Flores, Silvia and Gaytán-Lugo, Laura and Santana, Pedro and Crossa, Jose},
year = {2021},
month = {01},
pages = {},
title = {A review of deep learning applications for genomic selection},
volume = {22},
journal = {BMC Genomics},
doi = {10.1186/s12864-020-07319-x}
}

@ARTICLE{montesinos_lopez_2018,
  
AUTHOR={Montesinos-López, Osval A. and Montesinos-López, Abelardo and Tuberosa, Roberto and Maccaferri, Marco and Sciara, Giuseppe and Ammar, Karim and Crossa, José},   
	 
TITLE={Multi-Trait, Multi-Environment Genomic Prediction of Durum Wheat With Genomic Best Linear Unbiased Predictor and Deep Learning Methods},      
	
JOURNAL={Frontiers in Plant Science},      
	
VOLUME={10},      

PAGES={1311},     
	
YEAR={2019},      
	  
URL={https://www.frontiersin.org/article/10.3389/fpls.2019.01311},       
	
DOI={10.3389/fpls.2019.01311},      
	
ISSN={1664-462X},   
   
ABSTRACT={Although durum wheat (Triticum turgidum var. durum Desf.) is a minor cereal crop representing just 5–7\% of the world’s total wheat crop, it is a staple food in Mediterranean countries, where it is used to produce pasta, couscous, bulgur and bread. In this paper, we cover multi-trait prediction of grain yield (GY), days to heading (DH) and plant height (PH) of 270 durum wheat lines that were evaluated in 43 environments (country–location–year combinations) across a broad range of water regimes in the Mediterranean Basin and other locations. Multi-trait prediction analyses were performed by implementing a multi-trait deep learning model (MTDL) with a feed-forward network topology and a rectified linear unit activation function with a grid search approach for the selection of hyper-parameters. The results of the multi-trait deep learning method were also compared with univariate predictions of the genomic best linear unbiased predictor (GBLUP) method and the univariate counterpart of the multi-trait deep learning method (UDL). All models were implemented with and without the genotype × environment interaction term. We found that the best predictions were observed without the genotype × environment interaction term in the UDL and MTDL methods. However, under the GBLUP method, the best predictions were observed when the genotype × environment interaction term was taken into account. We also found that in general the best predictions were observed under the GBLUP model; however, the predictions of the MTDL were very similar to those of the GBLUP model. This result provides more evidence that the GBLUP model is a powerful approach for genomic prediction, but also that the deep learning method is a practical approach for predicting univariate and multivariate traits in the context of genomic selection.}
}


@article{montesinos-lopez_new_2019,
	title = {New {Deep} {Learning} {Genomic}-{Based} {Prediction} {Model} for {Multiple} {Traits} with {Binary}, {Ordinal}, and {Continuous} {Phenotypes}},
	volume = {9},
	copyright = {Copyright © 2019 Montesinos-López et al.. This is an open-access article distributed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.},
	issn = {2160-1836},
	url = {https://www.g3journal.org/content/9/5/1545},
	doi = {10.1534/g3.119.300585},
	abstract = {Multiple-trait experiments with mixed phenotypes (binary, ordinal and continuous) are not rare in animal and plant breeding programs. However, there is a lack of statistical models that can exploit the correlation between traits with mixed phenotypes in order to improve prediction accuracy in the context of genomic selection (GS). For this reason, when breeders have mixed phenotypes, they usually analyze them using univariate models, and thus are not able to exploit the correlation between traits, which many times helps improve prediction accuracy. In this paper we propose applying deep learning for analyzing multiple traits with mixed phenotype data in terms of prediction accuracy. The prediction performance of multiple-trait deep learning with mixed phenotypes (MTDLMP) models was compared to the performance of univariate deep learning (UDL) models. Both models were evaluated using predictors with and without the genotype × environment (G×E) interaction term (I and WI, respectively). The metric used for evaluating prediction accuracy was Pearson’s correlation for continuous traits and the percentage of cases correctly classified (PCCC) for binary and ordinal traits. We found that a modest gain in prediction accuracy was obtained only in the continuous trait under the MTDLMP model compared to the UDL model, whereas for the other traits (1 binary and 2 ordinal) we did not find any difference between the two models. In both models we observed that the prediction performance was better for WI than for I. The MTDLMP model is a good alternative for performing simultaneous predictions of mixed phenotypes (binary, ordinal and continuous) in the context of GS.},
	language = {en},
	number = {5},
	urldate = {2021-09-12},
	journal = {G3: Genes, Genomes, Genetics},
	author = {Montesinos-López, Osval A. and Martín-Vallejo, Javier and Crossa, José and Gianola, Daniel and Hernández-Suárez, Carlos M. and Montesinos-López, Abelardo and Juliana, Philomin and Singh, Ravi},
	month = may,
	year = {2019},
	pmid = {30858235},
	note = {Publisher: G3: Genes, Genomes, Genetics
Section: Genomic Prediction},
	keywords = {deep learning, Genomic Prediction, genomic selection, GenPred, mixed phenotypes (binary ordinal and continuous), multiple-trait, plant breeding, Shared Data Resources},
	pages = {1545--1556},
	file = {Full Text PDF:C\:\\Users\\vamsi\\Zotero\\storage\\6BGJ6R2W\\Montesinos-López et al. - 2019 - New Deep Learning Genomic-Based Prediction Model f.pdf:application/pdf;Snapshot:C\:\\Users\\vamsi\\Zotero\\storage\\LZ3QLRAL\\1545.html:text/html},
}

@article{bellot_2018,
author = {Bellot, Pau and Campos, Gustavo and Pérez-Enciso, Miguel},
year = {2018},
month = {08},
pages = {},
title = {Can Deep Learning Improve Genomic Prediction of Complex Human Traits?},
volume = {210},
journal = {Genetics},
doi = {10.1534/genetics.118.301298}
}

@article{windhausen_2012,
author = {Windhausen, Vanessa and Atlin, Gary and Hickey, John and Crossa, Jose and Jannink, Jean-Luc and Sorrells, Mark and Babu, Raman and Cairns, Jill and Tarekegne, Amsal and Semagn, Kassa and Beyene, Yoseph and Grudloyma, Pichet and Technow, Frank and Riedelsheimer, Christian and Melchinger, Albrecht},
year = {2012},
month = {11},
pages = {1427-36},
title = {Effectiveness of Genomic Prediction of Maize Hybrid Performance in Different Breeding Populations and Environments},
volume = {2},
journal = {G3 (Bethesda, Md.)},
doi = {10.1534/g3.112.003699}
}


@article{crossa_genomic_2014,
	title = {Genomic prediction in {CIMMYT} maize and wheat breeding programs},
	volume = {112},
	copyright = {2014 The Author(s)},
	issn = {1365-2540},
	url = {https://www.nature.com/articles/hdy201316},
	doi = {10.1038/hdy.2013.16},
	abstract = {Genomic selection (GS) has been implemented in animal and plant species, and is regarded as a useful tool for accelerating genetic gains. Varying levels of genomic prediction accuracy have been obtained in plants, depending on the prediction problem assessed and on several other factors, such as trait heritability, the relationship between the individuals to be predicted and those used to train the models for prediction, number of markers, sample size and genotype × environment interaction (GE). The main objective of this article is to describe the results of genomic prediction in International Maize and Wheat Improvement Center’s (CIMMYT’s) maize and wheat breeding programs, from the initial assessment of the predictive ability of different models using pedigree and marker information to the present, when methods for implementing GS in practical global maize and wheat breeding programs are being studied and investigated. Results show that pedigree (population structure) accounts for a sizeable proportion of the prediction accuracy when a global population is the prediction problem to be assessed. However, when the prediction uses unrelated populations to train the prediction equations, prediction accuracy becomes negligible. When genomic prediction includes modeling GE, an increase in prediction accuracy can be achieved by borrowing information from correlated environments. Several questions on how to incorporate GS into CIMMYT’s maize and wheat programs remain unanswered and subject to further investigation, for example, prediction within and between related bi-parental crosses. Further research on the quantification of breeding value components for GS in plant breeding populations is required.},
	language = {en},
	number = {1},
	urldate = {2021-09-12},
	journal = {Heredity},
	author = {Crossa, J. and Pérez, P. and Hickey, J. and Burgueño, J. and Ornella, L. and Cerón-Rojas, J. and Zhang, X. and Dreisigacker, S. and Babu, R. and Li, Y. and Bonnett, D. and Mathews, K.},
	month = jan,
	year = {2014},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Plant breeding
Subject\_term\_id: plant-breeding},
	pages = {48--60},
	file = {Full Text PDF:C\:\\Users\\vamsi\\Zotero\\storage\\ZM5WYA5N\\Crossa et al. - 2014 - Genomic prediction in CIMMYT maize and wheat breed.pdf:application/pdf;Snapshot:C\:\\Users\\vamsi\\Zotero\\storage\\EZKY4JM3\\hdy201316.html:text/html},
}

@article{haile_2018,
author = {Haile, Jemanesh and N'Diaye, Amidou and Clarke, Fran and Clarke, John and Knox, Ron and Rutkoski, Jessica and Bassi, Filippo and Pozniak, C.},
year = {2018},
month = {05},
pages = {},
title = {Genomic selection for grain yield and quality traits in durum wheat},
volume = {38},
journal = {Molecular Breeding},
doi = {10.1007/s11032-018-0818-x}
}


@article{du_genomic_2018,
	title = {Genomic selection using principal component regression},
	volume = {121},
	copyright = {2018 The Genetics Society},
	issn = {1365-2540},
	url = {https://www.nature.com/articles/s41437-018-0078-x},
	doi = {10.1038/s41437-018-0078-x},
	abstract = {Many statistical methods are available for genomic selection (GS) through which genetic values of quantitative traits are predicted for plants and animals using whole-genome SNP data. A large number of predictors with much fewer subjects become a major computational challenge in GS. Principal components regression (PCR) and its derivative, i.e., partial least squares regression (PLSR), provide a solution through dimensionality reduction. In this study, we show that PCR can perform better than PLSR in cross validation. PCR often requires extracting more components to achieve the maximum predictive ability than PLSR and thus may be associated with a higher computational cost. However, application of the HAT method (a strategy of describing the relationship between the fitted and observed response variables with a hat matrix) to PCR circumvents conventional cross validation in testing predictive ability, resulting in substantially improved computational efficiency over PLSR where cross validation is mandatory. Advantages of PCR over PLSR are illustrated with a simulated trait of a hypothetical population and four agronomical traits of a rice population. The benefit of using PCR in genomic selection is further demonstrated in an effort to predict 1000 metabolomic traits and 24,973 transcriptomic traits in the same rice population.},
	language = {en},
	number = {1},
	urldate = {2021-08-23},
	journal = {Heredity},
	author = {Du, Caroline and Wei, Julong and Wang, Shibo and Jia, Zhenyu},
	month = jul,
	year = {2018},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Plant breeding
Subject\_term\_id: plant-breeding},
	pages = {12--23},
	file = {Full Text PDF:C\:\\Users\\vamsi\\Zotero\\storage\\SBVHY83X\\Du et al. - 2018 - Genomic selection using principal component regres.pdf:application/pdf},
}

@article {Xu_rice_2014,
	author = {Xu, Shizhong and Zhu, Dan and Zhang, Qifa},
	title = {Predicting hybrid performance in rice using genomic best linear unbiased prediction},
	volume = {111},
	number = {34},
	pages = {12456--12461},
	year = {2014},
	doi = {10.1073/pnas.1413750111},
	publisher = {National Academy of Sciences},
	abstract = {Genomic prediction is a new field of quantitative genetics. Individual performance can be predicted using genome-wide markers before the phenotype is measured. Genomic prediction for hybrid performance is even more promising because genotype of a hybrid is predetermined by the parents. We propose a genomic best linear unbiased prediction to predict hybrid performance of rice, and incorporate dominance and epistasis into the prediction model. Simulation studies showed that predictability can be further improved after incorporating dominance and epistasis into the model. The new strategy of marker-guided hybrid prediction is called genomic hybrid breeding, and it represents a new technology that may potentially revolutionize hybrid breeding in agriculture.Genomic selection is an upgrading form of marker-assisted selection for quantitative traits, and it differs from the traditional marker-assisted selection in that markers in the entire genome are used to predict genetic values and the QTL detection step is skipped. Genomic selection holds the promise to be more efficient than the traditional marker-assisted selection for traits controlled by polygenes. Genomic selection for pure breed improvement is based on marker information and thus leads to cost-saving due to early selection before phenotypes are measured. When applied to hybrid breeding, genomic selection is anticipated to be even more efficient because genotypes of hybrids are predetermined by their inbred parents. Hybrid breeding has been an important tool to increase crop productivity. Here we proposed and applied an advanced method to predict hybrid performance, in which a subset of all potential hybrids is used as a training sample to predict trait values of all potential hybrids. The method is called genomic best linear unbiased prediction. The technology applied to hybrids is called genomic hybrid breeding. We used 278 randomly selected hybrids derived from 210 recombinant inbred lines of rice as a training sample and predicted all 21,945 potential hybrids. The average yield of top 100 selection shows a 16\% increase compared with the average yield of all potential hybrids. The new strategy of marker-guided prediction of hybrid yields serves as a proof of concept for a new technology that may potentially revolutionize hybrid breeding.},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/111/34/12456},
	eprint = {https://www.pnas.org/content/111/34/12456.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

@article{spindel2015genomic,
  title={Genomic selection and association mapping in rice (Oryza sativa): effect of trait genetic architecture, training population composition, marker number and statistical model on accuracy of rice genomic selection in elite, tropical rice breeding lines},
  author={Spindel, Jennifer and Begum, Hasina and Akdemir, Deniz and Virk, Parminder and Collard, Bertrand and Redona, Edilberto and Atlin, Gary and Jannink, Jean-Luc and McCouch, Susan R},
  journal={PLoS genetics},
  volume={11},
  number={2},
  pages={e1004982},
  year={2015},
  publisher={Public Library of Science San Francisco, CA USA}
}

@article{zeng2017rational,
  title={Rational design of high-yield and superior-quality rice},
  author={Zeng, Dali and Tian, Zhixi and Rao, Yuchun and Dong, Guojun and Yang, Yaolong and Huang, Lichao and Leng, Yujia and Xu, Jie and Sun, Chuan and Zhang, Guangheng and others},
  journal={Nature plants},
  volume={3},
  number={4},
  pages={1--5},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{burstin2015genetic,
  title={Genetic diversity and trait genomic prediction in a pea diversity panel},
  author={Burstin, Judith and Salloignon, Pauline and Chabert-Martinello, Marianne and Magnin-Robert, Jean-Bernard and Siol, Mathieu and Jacquin, Fran{\c{c}}oise and Chauveau, Aur{\'e}lie and Pont, Caroline and Aubert, Gr{\'e}goire and Delaitre, Catherine and others},
  journal={BMC genomics},
  volume={16},
  number={1},
  pages={1--17},
  year={2015},
  publisher={Springer}
}

@article{varshney2018accelerating,
  title={Accelerating genetic gains in legumes for the development of prosperous smallholder agriculture: integrating genomics, phenotyping, systems modelling and agronomy},
  author={Varshney, Rajeev K and Thudi, Mahendar and Pandey, Manish K and Tardieu, Francois and Ojiewo, Chris and Vadez, Vincent and Whitbread, Anthony M and Siddique, Kadambot HM and Nguyen, Henry T and Carberry, Peter S and others},
  journal={Journal of Experimental Botany},
  volume={69},
  number={13},
  pages={3293--3312},
  year={2018},
  publisher={Oxford University Press UK}
}

@article{okeke2017accuracies,
  title={Accuracies of univariate and multivariate genomic prediction models in African cassava},
  author={Okeke, Uche Godfrey and Akdemir, Deniz and Rabbi, Ismail and Kulakow, Peter and Jannink, Jean-Luc},
  journal={Genetics Selection Evolution},
  volume={49},
  number={1},
  pages={1--10},
  year={2017},
  publisher={BioMed Central}
}

@article{de_oliviera_2012,
author = {Oliveira, Eder and de Resende, Marcos Deon and Santos, Vanderlei and Ferreira, Claudia and Oliveira, Gilmara and Silva, Maiane and Oliveira, Luciana and Aguilar-Vildoso, Carlos},
year = {2012},
month = {09},
pages = {},
title = {Genome-wide selection in cassava},
volume = {187},
journal = {Euphytica},
doi = {10.1007/s10681-012-0722-0}
}

@article{roth2020genomic,
  title={Genomic prediction of fruit texture and training population optimization towards the application of genomic selection in apple},
  author={Roth, Morgane and Muranty, H{\'e}l{\`e}ne and Di Guardo, Mario and Guerra, Walter and Patocchi, Andrea and Costa, Fabrizio},
  journal={Horticulture research},
  volume={7},
  number={1},
  pages={1--14},
  year={2020},
  publisher={Nature Publishing Group}
}


@article{turnbull_iterative_2013,
	title = {Iterative selection using orthogonal regression techniques},
	volume = {6},
	issn = {1932-1872},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sam.11212},
	doi = {https://doi.org/10.1002/sam.11212},
	abstract = {High dimensional data are nowadays encountered in various branches of science. Variable selection techniques play a key role in analyzing high dimensional data. Generally two approaches for variable selection in the high dimensional data setting are considered—forward selection methods and penalization methods. In the former, variables are introduced in the model one at a time depending on their ability to explain variation and the procedure is terminated at some stage following some stopping rule. In penalization techniques such as the least absolute selection and shrinkage operator (LASSO), as optimization procedure is carried out with an added carefully chosen penalty function, so that the solutions have a sparse structure. Recently, the idea of penalized forward selection has been introduced. The motivation comes from the fact that the penalization techniques like the LASSO give rise to closed form expressions when used in one dimension, just like the least squares estimator. Hence one can repeat such a procedure in a forward selection setting until it converges. The resulting procedure selects sparser models than comparable methods without compromising on predictive power. However, when the regressor is high dimensional, it is typical that many predictors are highly correlated. We show that in such situations, it is possible to improve stability and computational efficiency of the procedure further by introducing an orthogonalization step. At each selection step, variables potentially available to be selected in the model are screened on the basis of their correlation with variables already in the model, thus preventing unnecessary duplication. The new strategy, called the Selection Technique in Orthogonalized Regression Models (STORM), turns out to be extremely successful in reducing the model dimension further and also leads to improved predicting power. We also consider an aggressive version of the STORM, where a potential predictor will be permanently removed from further consideration if its regression coefficient is estimated as zero at any stage. We shall carry out a detailed simulation study to compare the newly proposed method with existing ones and analyze a real dataset. © 2013 Wiley Periodicals, Inc. Statistical Analysis and Data Mining, 2013},
	language = {en},
	number = {6},
	urldate = {2020-12-04},
	journal = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
	author = {Turnbull, Bradley and Ghosal, Subhashis and Zhang, Hao Helen},
	year = {2013},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sam.11212},
	keywords = {forward selection, high dimensional regression, LASSO, orthogonalization},
	pages = {557--564},
	file = {Snapshot:C\:\\Users\\vamsi\\Zotero\\storage\\R5HGS2PV\\sam.html:text/html;Full Text PDF:C\:\\Users\\vamsi\\Zotero\\storage\\APIMWTG4\\Turnbull et al. - 2013 - Iterative selection using orthogonal regression te.pdf:application/pdf},
}

@article{ghosal_sparse_2016,
	title = {Sparse {Penalized} {Forward} {Selection} for {Support} {Vector} {Classification}},
	volume = {25},
	issn = {1061-8600},
	url = {https://doi.org/10.1080/10618600.2015.1023395},
	doi = {10.1080/10618600.2015.1023395},
	abstract = {We propose a new binary classification and variable selection technique especially designed for high-dimensional predictors. Among many predictors, typically, only a small fraction of them have significant impact on prediction. In such a situation, more interpretable models with better prediction accuracy can be obtained by variable selection along with classification. By adding an ℓ1-type penalty to the loss function, common classification methods such as logistic regression or support vector machines (SVM) can perform variable selection. Existing penalized SVM methods all attempt to jointly solve all the parameters involved in the penalization problem altogether. When data dimension is very high, the joint optimization problem is very complex and involves a lot of memory allocation. In this article, we propose a new penalized forward search technique that can reduce high-dimensional optimization problems to one-dimensional optimization by iterating the selection steps. The new algorithm can be regarded as a forward selection version of the penalized SVM and its variants. The advantage of optimizing in one dimension is that the location of the optimum solution can be obtained with intelligent search by exploiting convexity and a piecewise linear or quadratic structure of the criterion function. In each step, the predictor that is most able to predict the outcome is chosen in the model. The search is then repeatedly used in an iterative fashion until convergence occurs. Comparison of our new classification rule with ℓ1-SVM and other common methods show very promising performance, in that the proposed method leads to much leaner models without compromising misclassification rates, particularly for high-dimensional predictors.},
	number = {2},
	urldate = {2020-12-04},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Ghosal, Subhashis and Turnbull, Bradley and Zhang, Hao Helen and Hwang, Wook Yeon},
	month = apr,
	year = {2016},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/10618600.2015.1023395},
	keywords = {High dimension, Penalization, Sparsity, SVM, Variable selection},
	pages = {493--514},
	file = {Full Text PDF:C\:\\Users\\vamsi\\Zotero\\storage\\V6SM4I5Y\\Ghosal et al. - 2016 - Sparse Penalized Forward Selection for Support Vec.pdf:application/pdf},
}

@article{ghosal_first_2009,
	title = {{FIRST}: {Combining} forward iterative selection and shrinkage in high dimensional sparse linear regression},
	volume = {2},
	issn = {19387989, 19387997},
	shorttitle = {{FIRST}},
	url = {http://www.intlpress.com/site/pub/pages/journals/items/sii/content/vols/0002/0003/a007/},
	doi = {10.4310/SII.2009.v2.n3.a7},
	language = {en},
	number = {3},
	urldate = {2020-12-07},
	journal = {Statistics and Its Interface},
	author = {Ghosal, Subhashis and Hwang, Wook Yeon and Zhang, Hao Helen},
	year = {2009},
	pages = {341--348},
	file = {Ghosal et al. - 2009 - FIRST Combining forward iterative selection and s.pdf:C\:\\Users\\vamsi\\Zotero\\storage\\WH7GIR6P\\Ghosal et al. - 2009 - FIRST Combining forward iterative selection and s.pdf:application/pdf},
}

@article{perez2014genome,
  title={Genome-wide regression and prediction with the BGLR statistical package},
  author={P{\'e}rez, Paulino and de Los Campos, Gustavo},
  journal={Genetics},
  volume={198},
  number={2},
  pages={483--495},
  year={2014},
  publisher={Oxford University Press}
}


@article{arouisse_improving_2021,
	title = {Improving {Genomic} {Prediction} {Using} {High}-{Dimensional} {Secondary} {Phenotypes}},
	volume = {12},
	issn = {1664-8021},
	url = {https://www.frontiersin.org/article/10.3389/fgene.2021.667358},
	doi = {10.3389/fgene.2021.667358},
	abstract = {In the past decades, genomic prediction has had a large impact on plant breeding. Given the current advances of high-throughput phenotyping and sequencing technologies, it is increasingly common to observe a large number of traits, in addition to the target trait of interest. This raises the important question whether these additional or “secondary” traits can be used to improve genomic prediction for the target trait. With only a small number of secondary traits, this is known to be the case, given sufficiently high heritabilities and genetic correlations. Here we focus on the more challenging situation with a large number of secondary traits, which is increasingly common since the arrival of high-throughput phenotyping. In this case, secondary traits are usually incorporated through additional relatedness matrices. This approach is however infeasible when secondary traits are not measured on the test set, and cannot distinguish between genetic and non-genetic correlations. An alternative direction is to extend the classical selection indices using penalized regression. So far, penalized selection indices have not been applied in a genomic prediction setting, and require plot-level data in order to reliably estimate genetic correlations. Here we aim to overcome these limitations, using two novel approaches. Our first approach relies on a dimension reduction of the secondary traits, using either penalized regression or random forests (LS-BLUP/RF-BLUP). We then compute the bivariate GBLUP with the dimension reduction as secondary trait. For simulated data (with available plot-level data), we also use bivariate GBLUP with the penalized selection index as secondary trait (SI-BLUP). In our second approach (GM-BLUP), we follow existing multi-kernel methods but replace secondary traits by their genomic predictions, with the advantage that genomic prediction is also possible when secondary traits are only measured on the training set. For most of our simulated data, SI-BLUP was most accurate, often closely followed by RF-BLUP or LS-BLUP. In real datasets, involving metabolites in Arabidopsis and transcriptomics in maize, no method could substantially improve over univariate prediction when secondary traits were only available on the training set. LS-BLUP and RF-BLUP were most accurate when secondary traits were available also for the test set.},
	urldate = {2021-08-16},
	journal = {Frontiers in Genetics},
	author = {Arouisse, Bader and Theeuwen, Tom P. J. M. and van Eeuwijk, Fred A. and Kruijer, Willem},
	year = {2021},
	pages = {715},
	file = {Full Text PDF:C\:\\Users\\vamsi\\Zotero\\storage\\IWAC4ZUE\\Arouisse et al. - 2021 - Improving Genomic Prediction Using High-Dimensiona.pdf:application/pdf},
}

@article{lopez-cruz_regularized_2020,
	title = {Regularized selection indices for breeding value prediction using hyper-spectral image data},
	volume = {10},
	copyright = {2020 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-020-65011-2},
	doi = {10.1038/s41598-020-65011-2},
	abstract = {High-throughput phenotyping (HTP) technologies can produce data on thousands of phenotypes per unit being monitored. These data can be used to breed for economically and environmentally relevant traits (e.g., drought tolerance); however, incorporating high-dimensional phenotypes in genetic analyses and in breeding schemes poses important statistical and computational challenges. To address this problem, we developed regularized selection indices; the methodology integrates techniques commonly used in high-dimensional phenotypic regressions (including penalization and rank-reduction approaches) into the selection index (SI) framework. Using extensive data from CIMMYT’s (International Maize and Wheat Improvement Center) wheat breeding program we show that regularized SIs derived from hyper-spectral data offer consistently higher accuracy for grain yield than those achieved by standard SIs, and by vegetation indices commonly used to predict agronomic traits. Regularized SIs offer an effective approach to leverage HTP data that is routinely generated in agriculture; the methodology can also be used to conduct genetic studies using high-dimensional phenotypes that are often collected in humans and model organisms including body images and whole-genome gene expression profiles.},
	language = {en},
	number = {1},
	urldate = {2021-09-10},
	journal = {Scientific Reports},
	author = {Lopez-Cruz, Marco and Olson, Eric and Rovere, Gabriel and Crossa, Jose and Dreisigacker, Susanne and Mondal, Suchismita and Singh, Ravi and Campos, Gustavo de los},
	month = may,
	year = {2020},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Quantitative trait;Statistical methods
Subject\_term\_id: quantitative-trait;statistical-methods},
	pages = {8195},
	file = {Full Text PDF:C\:\\Users\\vamsi\\Zotero\\storage\\ITH8K8HQ\\Lopez-Cruz et al. - 2020 - Regularized selection indices for breeding value p.pdf:application/pdf;Snapshot:C\:\\Users\\vamsi\\Zotero\\storage\\D3U3P9UV\\s41598-020-65011-2.html:text/html},
}

@article{sandhu_combining_2021,
	title = {Combining {Genomic} and {Phenomic} {Information} for {Predicting} {Grain} {Protein} {Content} and {Grain} {Yield} in {Spring} {Wheat}},
	volume = {12},
	issn = {1664-462X},
	url = {https://www.frontiersin.org/article/10.3389/fpls.2021.613300},
	doi = {10.3389/fpls.2021.613300},
	abstract = {Genomics and high throughput phenomics have the potential to revolutionize the field of wheat (Triticum aestivum L.) breeding. Genomic selection (GS) has been used for predicting various quantitative traits in wheat, especially grain yield. However, there are few GS studies for grain protein content (GPC), which is a crucial quality determinant. Incorporation of secondary correlated traits in GS models has been demonstrated to improve accuracy. The objectives of this research were to compare performance of single and multi-trait GS models for predicting GPC and grain yield in wheat and to identify optimal growth stages for collecting secondary traits. We used 650 recombinant inbred lines from a spring wheat nested association mapping (NAM) population. The population was phenotyped over 3 years (2014–2016), and spectral information was collected at heading and grain filling stages. The ability to predict GPC and grain yield was assessed using secondary traits, univariate, covariate, and multivariate GS models for within and across cycle predictions. Our results indicate that GS accuracy increased by an average of 12\% for GPC and 20\% for grain yield by including secondary traits in the models. Spectral information collected at heading was superior for predicting GPC, whereas grain yield was more accurately predicted during the grain filling stage. Green normalized difference vegetation index had the largest effect on the prediction of GPC either used individually or with multiple indices in the GS models. An increased prediction ability for GPC and grain yield with the inclusion of secondary traits demonstrates the potential to improve the genetic gain per unit time and cost in wheat breeding.},
	urldate = {2021-09-12},
	journal = {Frontiers in Plant Science},
	author = {Sandhu, Karansher S. and Mihalyov, Paul D. and Lewien, Megan J. and Pumphrey, Michael O. and Carter, Arron H.},
	year = {2021},
	pages = {170},
	file = {Full Text PDF:C\:\\Users\\vamsi\\Zotero\\storage\\JIY49U75\\Sandhu et al. - 2021 - Combining Genomic and Phenomic Information for Pre.pdf:application/pdf},
}

@article{schrag_beyond_2018,
	title = {Beyond {Genomic} {Prediction}: {Combining} {Different} {Types} of \textit{omics} {Data} {Can} {Improve} {Prediction} of {Hybrid} {Performance} in {Maize}},
	volume = {208},
	issn = {1943-2631},
	shorttitle = {Beyond {Genomic} {Prediction}},
	url = {https://academic.oup.com/genetics/article/208/4/1373/6084222},
	doi = {10.1534/genetics.117.300374},
	abstract = {The ability to predict the agronomic performance of single-crosses with high precision is essential for selecting superior candidates for hybrid breeding. With recent technological advances, thousands of new parent lines, and, consequently, millions of new hybrid combinations are possible in each breeding cycle, yet only a few hundred can be produced and phenotyped in multi-environment yield trials. Well established prediction approaches such as best linear unbiased prediction (BLUP) using pedigree data and whole-genome prediction using genomic data are limited in capturing epistasis and interactions occurring within and among downstream biological strata such as transcriptome and metabolome. Because mRNA and small RNA (sRNA) sequences are involved in transcriptional, translational and post-translational processes, we expect them to provide information inﬂuencing several biological strata. However, using sRNA data of parent lines to predict hybrid performance has not yet been addressed. Here, we gathered genomic, transcriptomic (mRNA and sRNA) and metabolomic data of parent lines to evaluate the ability of the data to predict the performance of untested hybrids for important agronomic traits in grain maize. We found a considerable interaction for predictive ability between predictor and trait, with mRNA data being a superior predictor for grain yield and genomic data for grain dry matter content, while sRNA performed relatively poorly for both traits. Combining mRNA and genomic data as predictors resulted in high predictive abilities across both traits and combining other predictors improved prediction over that of the individual predictors alone. We conclude that downstream “omics” can complement genomics for hybrid prediction, and, thereby, contribute to more efﬁcient selection of hybrid candidates.},
	language = {en},
	number = {4},
	urldate = {2021-09-16},
	journal = {Genetics},
	author = {Schrag, Tobias A and Westhues, Matthias and Schipprack, Wolfgang and Seifert, Felix and Thiemann, Alexander and Scholten, Stefan and Melchinger, Albrecht E},
	month = apr,
	year = {2018},
	pages = {1373--1385},
	file = {genetics1373.pdf:C\:\\Users\\vamsi\\Zotero\\storage\\Y694VQFL\\genetics1373.pdf:application/pdf},
}

@article{zou2006adaptive,
  title={The adaptive lasso and its oracle properties},
  author={Zou, Hui},
  journal={Journal of the American statistical association},
  volume={101},
  number={476},
  pages={1418--1429},
  year={2006},
  publisher={Taylor \& Francis}
}
